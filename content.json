{"meta":{"title":"数大招疯","subtitle":null,"description":null,"author":"XueMin Zhang","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"大数定律","slug":"数据科学/大数定律","date":"2018-11-05T08:48:05.000Z","updated":"2018-11-20T14:15:52.976Z","comments":true,"path":"2018/11/05/数据科学/大数定律/","link":"","permalink":"http://yoursite.com/2018/11/05/数据科学/大数定律/","excerpt":"","text":"大数定律分为弱大数定律和强大数定律。强、弱大数定律都是在说：随着样本数的增大，用样本的平均数来估计总体的平均数，是靠谱的。 大数定律的条件 强、弱大数定律的前提条件一样：要求独立同分布i.i.d的随机序列，要求其期望存在。 什么是强、弱大数定律弱大数定律比较早被证明出来，弱大数定律表示样本均值“依概率收敛”于总体均值；而强大数定律是比较晚被证明出来的，它证明了样本均值可以“以概率为1收敛”于总体均值。简单的来说，就是数学家先证明了弱大数定律，后来在没有改变前提的情况下把弱大数定律推进了一步，得到了更厉害的强大数定律。 强、弱大数据定律的区别弱、强大数定律的区别在于，前者是“依概率收敛(convergence in probability)”，后者是“几乎确定收敛(almost surely convergence)或以概率为1收敛、几乎处处收敛”。后者比前者强，满足后者的必定满足前者，而满足前者的未必满足后者。 依概率收敛的例子：考虑下图，图中的每条线都代表一个数列，虚线表示一个非常小的区间。总的来说每个数列都越来越趋近0，且大部分时候不会超过虚线所表示的小边界，但是，偶尔会有一两条线超过虚线、然后再回到虚线之内。而且我们不能保证，有没有哪一个数列会在未来再次超出虚线的范围然后再回来——虽然概率很小。注意虚线的范围可以是任意小的实数，此图中大约是，可以把这个边界缩小到，甚至，随你喜欢，这个性质始终存在。 几乎处处收敛的例子：图中的黑线表示一个随机数列，这个数列在大约n=200之后进入了一个我们定的小边界（用虚线表示），之后我们可以确定，它再也不会超出虚线所表示的边界（超出这个边界的概率是0）。跟上面的例子一样，虚线所表示的边界可以定得任意小，而一定会有一个n值，当这个数列超过了n值之后，超出这个边界的概率就是0了。 内容来源：https://www.zhihu.com/question/21110761 ,作者：runze Zheng","categories":[{"name":"数据科学","slug":"数据科学","permalink":"http://yoursite.com/categories/数据科学/"}],"tags":[{"name":"概率","slug":"概率","permalink":"http://yoursite.com/tags/概率/"}],"keywords":[{"name":"数据科学","slug":"数据科学","permalink":"http://yoursite.com/categories/数据科学/"}]},{"title":"概率导论：二、离散随机变量","slug":"数据科学/概率导论：二、离散随机变量","date":"2018-09-17T08:48:05.000Z","updated":"2018-11-17T14:29:22.906Z","comments":true,"path":"2018/09/17/数据科学/概率导论：二、离散随机变量/","link":"","permalink":"http://yoursite.com/2018/09/17/数据科学/概率导论：二、离散随机变量/","excerpt":"","text":"","categories":[{"name":"数据科学","slug":"数据科学","permalink":"http://yoursite.com/categories/数据科学/"}],"tags":[{"name":"概率","slug":"概率","permalink":"http://yoursite.com/tags/概率/"}],"keywords":[{"name":"数据科学","slug":"数据科学","permalink":"http://yoursite.com/categories/数据科学/"}]},{"title":"概率导论：一、样本空间与概率","slug":"数据科学/概率导论：一、样本空间与概率","date":"2018-08-05T08:48:05.000Z","updated":"2018-11-17T14:29:00.822Z","comments":true,"path":"2018/08/05/数据科学/概率导论：一、样本空间与概率/","link":"","permalink":"http://yoursite.com/2018/08/05/数据科学/概率导论：一、样本空间与概率/","excerpt":"","text":"","categories":[{"name":"数据科学","slug":"数据科学","permalink":"http://yoursite.com/categories/数据科学/"}],"tags":[{"name":"概率","slug":"概率","permalink":"http://yoursite.com/tags/概率/"}],"keywords":[{"name":"数据科学","slug":"数据科学","permalink":"http://yoursite.com/categories/数据科学/"}]},{"title":"如何避免HBase写入过快引起的各种问题","slug":"大数据/HBase/如何避免HBase写入过快引起的各种问题","date":"2018-04-12T08:48:05.000Z","updated":"2018-11-29T11:55:03.104Z","comments":true,"path":"2018/04/12/大数据/HBase/如何避免HBase写入过快引起的各种问题/","link":"","permalink":"http://yoursite.com/2018/04/12/大数据/HBase/如何避免HBase写入过快引起的各种问题/","excerpt":"","text":"首先我们简单回顾下整个写入流程1client api ==&gt; RPC ==&gt; server IPC ==&gt; RPC queue ==&gt; RPC handler ==&gt; write WAL ==&gt; write memstore ==&gt; flush to filesystem 整个写入流程从客户端调用API开始，数据会通过protobuf编码成一个请求，通过scoket实现的IPC模块被送达server的RPC队列中。最后由负责处理RPC的handler取出请求完成写入操作。写入会先写WAL文件，然后再写一份到内存中，也就是memstore模块，当满足条件时，memstore才会被flush到底层文件系统，形成HFile。 当写入过快时会遇见什么问题？写入过快时，memstore的水位会马上被推高。你可能会看到以下类似日志：1RegionTooBusyException: Above memstore limit, regionName=xxxxx ... 这个是Region的memstore占用内存大小超过正常的4倍，这时候会抛异常，写入请求会被拒绝，客户端开始重试请求。当达到128M的时候会触发flush memstore，当达到128M * 4还没法触发flush时候会抛异常来拒绝写入。两个相关参数的默认值如下：12hbase.hregion.memstore.flush.size=128Mhbase.hregion.memstore.block.multiplier=4 或者这样的日志：12regionserver.MemStoreFlusher: Blocking updates on hbase.example.host.com,16020,1522286703886: the global memstore size 1.3 G is &gt;= than blocking 1.3 G sizeregionserver.MemStoreFlusher: Memstore is above high water mark and block 528ms 这是所有region的memstore内存总和开销超过配置上限，默认是配置heap的40%，这会导致写入被阻塞。目的是等待flush的线程把内存里的数据flush下去，否则继续允许写入memestore会把内存写爆12hbase.regionserver.global.memstore.upperLimit=0.4 # 较旧版本，新版本兼容hbase.regionserver.global.memstore.size=0.4 # 新版本 当写入被阻塞，队列会开始积压，如果运气不好最后会导致OOM，你可能会发现JVM由于OOM crash或者看到如下类似日志：12ipc.RpcServer: /192.168.x.x:16020 is unable to read call parameter from client 10.47.x.xjava.lang.OutOfMemoryError: Java heap space HBase这里我认为有个很不好的设计，捕获了OOM异常却没有终止进程。这时候进程可能已经没法正常运行下去了，你还会在日志里发现很多其它线程也抛OOM异常。比如stop可能根本stop不了，RS可能会处于一种僵死状态。 如何避免RS OOM？一种是加快flush速度：123hbase.hstore.blockingWaitTime = 90000 mshbase.hstore.flusher.count = 2hbase.hstore.blockingStoreFiles = 10 当达到hbase.hstore.blockingStoreFiles配置上限时，会导致flush阻塞等到compaction工作完成。阻塞时间是hbase.hstore.blockingWaitTime，可以改小这个时间。hbase.hstore.flusher.count可以根据机器型号去配置，可惜这个数量不会根据写压力去动态调整，配多了，非导入数据多场景也没用，改配置还得重启。 同样的道理，如果flush加快，意味这compaction也要跟上，不然文件会越来越多，这样scan性能会下降，开销也会增大。12hbase.regionserver.thread.compaction.small = 1hbase.regionserver.thread.compaction.large = 1 增加compaction线程会增加CPU和带宽开销，可能会影响正常的请求。如果不是导入数据，一般而言是够了。好在这个配置在云HBase内是可以动态调整的，不需要重启。 上述配置都需要人工干预，如果干预不及时server可能已经OOM了，这时候有没有更好的控制方法？1hbase.ipc.server.max.callqueue.size = 1024 * 1024 * 1024 # 1G 直接限制队列堆积的大小。当堆积到一定程度后，事实上后面的请求等不到server端处理完，可能客户端先超时了。并且一直堆积下去会导致OOM，1G的默认配置需要相对大内存的型号。当达到queue上限，客户端会收到CallQueueTooBigException 然后自动重试。通过这个可以防止写入过快时候把server端写爆，有一定反压作用。线上使用这个在一些小型号稳定性控制上效果不错。 转载：https://my.oschina.net/u/3770361/blog/1793324","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"2017年个人总结","slug":"其他/2017年个人总结","date":"2018-02-02T08:48:05.000Z","updated":"2018-09-12T02:45:29.291Z","comments":true,"path":"2018/02/02/其他/2017年个人总结/","link":"","permalink":"http://yoursite.com/2018/02/02/其他/2017年个人总结/","excerpt":"","text":"恍恍惚惚，18年已过一个月。作为严重的拖延症患者，近两周才对17年做了整体的回顾，做了组内的绩效沟通，今天也抽空做下个人方面的总结。 对于工作，与往年比较雷同，有项目成功的喜悦，也有项目延期、流产的无奈与自责；有与Team小伙伴一起成长的开心和自豪，也有小伙伴分别的难过和惆怅；有与同事协同攻关的艰辛与幸福，也有与同事争吵、摩擦后的委屈与懊悔；有苛求完美、力求成功的踌躇满志，也有思想上的“开小差”与面对选择时的纠结与迷惘…… 对于个人成长，16年年底曾给自己设立了两个个人目标，分别是读书与跑步。回头看下两个目标： 关于读书，一个字总结：乱一个是读书介质乱，有纸质书、有kindle、有ipad、有熊猫学院、有微信读书。 一个是书的“类型”乱，有工作周边的《看穿一切数字的统计学》、《白话大数据与机器学习》、《大数据之路》；有比较热门的《未来简史》，有传记、历史类 的《杜月笙传》、《朱德传》、《曾国藩正面与侧面》、《让你爱不释手的极简中国史》、《地球的红飘带》；有推理名家东野圭吾的《梦幻花》、《白夜行》、《幻夜》、《假面饭店》以及热播电视剧《人民的名义》；也读了梁晓声的《年轮》和蕾秋·乔伊斯的《一个人的朝圣》以及周小波的《黄金时代》、《三十而立》、《逝水流年》。当然，也有因为难度及兴趣问题还未读完的一些书，如《机器学习（西瓜书）》、《数据的本质》、《金字塔原理》、《腾讯传》等。 关于跑步，一个字总结：懒16年底定的目标17年跑步500公里，实际上一年下来却跑步不足300公里。也找了没有达到目标的原因，其中有鼻子的问题，但最大的原因在于本身的惰性。曾在夏季夜跑过，也在周六日尝试过晨跑，但都没有坚持下来，所以，18年这个是需要自己去克服的问题，加强锻炼身体的意识，养成一个健康生活的习惯。 整体来讲，对17年读书目标完成度比较满意，计划10本，实际完成19本。虽然跑步目标完成度不好，但也越来越意识到身体健康的重要性，介于之前小病都不是病，抗一抗总会过去的认知，一直透支健康，最终留下鼻炎的后患，也切身体会到5年前同事所说 的“每次呼吸都是痛”。很多时候工作拼的是你储备的知识也有你的体能，身体很重要，且行且珍惜。 在此也定下18年的个人目标： 读书：20本非技术、工具书 跑步：年跑量达到600公里 写文章：每月至少一篇原创或者翻译的文章 减肥：体检时候不再属于偏胖体质","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/tags/其他/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}]},{"title":"数据库发展史","slug":"大数据/数据库发展史","date":"2017-10-29T08:48:05.000Z","updated":"2020-03-29T10:43:48.656Z","comments":true,"path":"2017/10/29/大数据/数据库发展史/","link":"","permalink":"http://yoursite.com/2017/10/29/大数据/数据库发展史/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/大数据/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"CarbonData 编译打包","slug":"大数据/CarbonData 编译打包","date":"2017-09-29T08:48:05.000Z","updated":"2018-09-12T07:26:12.591Z","comments":true,"path":"2017/09/29/大数据/CarbonData 编译打包/","link":"","permalink":"http://yoursite.com/2017/09/29/大数据/CarbonData 编译打包/","excerpt":"","text":"背景CarbonData是什么？官网说明：Apache CarbonData is an indexed columnar data format for fast analytics on big data platform, e.g. Apache Hadoop, Apache Spark, etc.翻译过来即：Apache CarbonData 是一个含索引的列式数据格式，用于在如Apach Hadoop，Apach Spark等大数据平台上实现快速分析。 本想迅速试用一下，确发现官网没提供编译好的安装包，所以本篇文档记录下编译、打包的方法及过程。 一、基础条件 Unix类操作系统(Linux，Mac OS X) Git Maven （建议使用3.3以上版本） Java 7 或者 8 Thrift (0.9.3及以上） 二、Thrift 安装其他基础组件都比较容易安装，这里分别记录下Linux、Mac OS X下Thrift的安装步骤 1、Linux系统环境(CentOS 6.5，root账号执行) 安装autoconf123456tar xvf autoconf-2.69.tar.gzcd autoconf-2.69./configure --prefix=/usrmakesudo make installcd .. 安装automake1234567wget http://ftp.gnu.org/gnu/automake/automake-1.14.tar.gztar xvf automake-1.14.tar.gzcd automake-1.14./configure --prefix=/usrmakesudo make installcd .. 安装bison1234567wget http://ftp.gnu.org/gnu/bison/bison-2.5.1.tar.gztar xvf bison-2.5.1.tar.gzcd bison-2.5.1./configure --prefix=/usrmakesudo make installcd .. 添加C++语言依赖库1yum -y install libevent-devel zlib-devel openssl-devel 安装boost(1.53以上)12345wget http://sourceforge.net/projects/boost/files/boost/1.53.0/boost_1_53_0.tar.gztar xvf boost_1_53_0.tar.gzcd boost_1_53_0./bootstrap.shsudo ./b2 install 安装Thrift123456cd thriftgit checkout 0.9.3./bootstrap.sh./configure --with-lua=nomakesudo make install 2、Mac系统环境(OS X 10.11.4)查看依赖 brew info thrift 安装依赖1brew install boost openssl libevent 使用安装包安装指定版本thrift 12345wget http://archive.apache.org/dist/thrift/0.9.3/thrift-0.9.3.tar.gztar -xcv thrift-0.9.3.tar.gzcd thrift-0.9.3./configuremake &amp;&amp; make install 3、安装检验1thrift -version 三、下载编译CarbonData源码1、下载源码1git clone https://github.com/apache/carbondata.git 2、编译打包 查看pom.xml内容发现，默认基于spark-2.1.0编译，上翻也有id为hadoop-2.2.0，hadoop-2.7.0相关profile，可以根据需求随意修改编译。 3、笔者使用：1mvn clean package -DskipTests -Pspark-1.6 -Dspark.version=1.6.3 -Phadoop-2.2.0 -Dhadoop.version=2.6.0 4、执行结果 如图表示编译成功，大概需要几分钟的时间。成功后会在assembly/target/scala-2.10/目录下生成carbondata_2.10-1.1.1-shade-hadoop2.6.0.jar 包，到此编译完成 四、遇到的问题和建议1、编译依赖thrift 开始下载源码后，直接无脑编译出错 2、建议编译特定的版本12git taggit checkout apache-carbondata-1.1.1-rc1 3、建议配置使用国内maven源123456&lt;mirror&gt;&lt;id&gt;nexus&lt;/id&gt;&lt;name&gt;nexus&lt;/name&gt;&lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;&lt;mirrorOf&gt;*&lt;/mirrorOf&gt;&lt;/mirror&gt; 4、期间遇到的一些异常12345678910111213141516myProject/carbondata/integration/spark/src/main/scala/org/apache/spark/sql/execution/command/carbonTableSchema.scala:179: error: value getOrDefault is not a member of java.util.Map[String,String][INFO] possible cause: maybe a semicolon is missing before `value getOrDefault&apos;?[INFO] .getOrDefault(&quot;sort_scope&quot;, CarbonCommonConstants.LOAD_SORT_SCOPE_DEFAULT)[INFO] ^[ERROR]myProject/carbondata/integration/spark/src/main/scala/org/apache/spark/sql/execution/command/carbonTableSchema.scala:453: error: value getOrDefault is not a member of java.util.Map[String,String][INFO] tableProperties.getOrDefault(&quot;sort_scope&quot;, sortScopeDefault)[INFO] ^[ERROR] /myProject/carbondata/integration/spark/src/main/scala/org/apache/spark/sql/execution/command/carbonTableSchema.scala:911: error: value getOrDefault is not a member of java.util.Map[String,String][INFO] .getTableProperties.getOrDefault(&quot;sort_scope&quot;, CarbonCommonConstants[INFO] ^[WARNING] two warnings found[ERROR] Failed to execute goal org.scala-tools:maven-scala-plugin:2.15.2:compile (default) on project carbondata-spark: wrap: org.apache.commons.exec.ExecuteException: Process exited with an error: 1(Exit value: 1) -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.scala-tools:maven-scala-plugin:2.15.2:compile (default) on project carbondata-spark: wrap: org.apache.commons.exec.ExecuteException: Process exited with an error: 1(Exit value: 1) ```","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/tags/其他/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}]},{"title":"如何优雅的关闭Spark Streaming作业","slug":"大数据/Spark/如何优雅的关闭Spark Streaming作业","date":"2017-06-20T08:48:05.000Z","updated":"2018-11-29T11:57:06.594Z","comments":true,"path":"2017/06/20/大数据/Spark/如何优雅的关闭Spark Streaming作业/","link":"","permalink":"http://yoursite.com/2017/06/20/大数据/Spark/如何优雅的关闭Spark Streaming作业/","excerpt":"","text":"Spark Streaming 应用定位是长期执行的。但如何优雅的关闭它，使正在被处理的消息在作业停止前被妥善处理？很多博文建议我们必须通过JVM关闭的钩子，可在此 查看相关代码。但是，这个方法在新的Spark版本（1.4版本之后）中不能正常工作，并且会引起死锁情况。 目前有两种方式去优雅的关闭Spark Streaming作业。第一种方法是设置spark.streaming.stopGracefullyOnShutdown参数值为true（默认是false）。这个参数在解决Spark优雅关闭的issue中引入。开发者不再需要去调用ssc.stop()函数，只需要向Driver发送SIGTERM信号。在实践中，我们需要如下操作： 在Spark UI上找到Driver进程运行在哪个节点。在Yarn Cluster部署模式下，Driver进程和AM运行在同一个Container。登陆运行Driver的节点，并且执行ps -ef |grep java |grep ApplicationMaster 去找到进程ID。请注意，你搜索的字符串可能会因为作业或者环境等原因不同。执行kill -SIGTERM 命令，发送SIGTERM信号给进程。在Spark Driver接收到SIGTERM信号后，你会在日志中看到类似如下的消息： 12345617/02/02 01:31:35 ERROR yarn.ApplicationMaster: RECEIVED SIGNAL 15: SIGTERM*17/02/02 01:31:35 INFO streaming.StreamingContext: Invoking stop(stopGracefully=true) from shutdown hook...17/02/02 01:31:45 INFO streaming.StreamingContext: StreamingContext stopped successfully**17/02/02 01:31:45 INFO spark.SparkContext: Invoking stop() from shutdown hook...17/02/02 01:31:45 INFO spark.SparkContext: Successfully stopped SparkContext...17/02/02 01:31:45 INFO util.ShutdownHookManager: Shutdown hook called* 需要注意，默 spark.yarn.maxAppAttempts默认使用Yarn的yarn.resourcemanager.am.max-attempts的值。而yarn.resourcemanager.am.max-attempts值默认为2。因此，在执行kill命令AM第一次停止后，Yarn将会自动启动另一个AM/Driver。你需要第二次kill掉它。你可以在spark-submit设置–conf spark.yarn.maxAppAttempts=1 ，但是你必须考虑清楚，因为如此配置后Driver失败后将没机会重试。 你不能使用yarn application -kill 去kill作业。这个命令不会发送SIGTERM信号给container，而是几乎同时发送SIGKILL信号。SIGTERM和SIGKILL之间的时间间隔可以使用yarn.nodemanager.sleep-delay-before-sigkill.ms (默认 250)去配置。当然，你可以增大该值，但是， 在一定程度上，即使我调整到60000（1分钟），它仍然不起作用。作业的containers几乎是立即被kill掉，并且日志中仅包含如下内容： 1217/02/02 12:12:27 ERROR yarn.ApplicationMaster: RECEIVED SIGNAL 15: SIGTERM*17/02/02 12:12:27 INFO streaming.StreamingContext: Invoking stop(stopGracefully=true) from shutdown hook* 所以，我不建议使用yarn application -kill 命令去发送SIGTERM信号。 第二个解决方案是以某种方式通知Spark Streaming应用它需要优雅的关闭，而不是使用SIGTERM信号。一种方式是在HDFS上放一个标识文件，Spark Streaming应用周期性的去检测它。如果标识文件存在了，就调用scc.stop(true, true) 。第一个true意思是Spark context需要被停止。第二个true意思是需要优雅的关闭，允许正在处理的消息完成。 至关重要的是，不要在micro-batch的代码中调用ssc.stop(true, true)，试想一下，如果你在微批代码中调用ssc.stop(true, true)，它将等待所有正在被处理的消息完成，包括当前正在执行的微批。但是，当前的微批不会结束，直到ssc.stop(true, true)结束返回。这是一种死锁的情况。所以，你必须在另一个线程中执行标识文件检测和调用ssc.stop(true, true)。我在github上放了一个简单的样例，此样例里我在mian线程中在ssc.start()后执行检测和调用ssc.stop() 。你可以在这里找到源码。当然，使用HDFS标识文件仅仅是一种方法，其他可选择的方法有使用一个单独的线程监听一个socket，启动一个RESTful服务等等。 期待在将来的release中，Spark会考虑更优雅的方案。比如，在Spark UI中可以增加一个按钮，去优雅的停止Spark Streaming作业，这样，我们就不需要凭借定制化的编码或者使用PID和SIGTERM信号了。 翻译：http://blog.parseconsulting.com/2017/02/how-to-shutdown-spark-streaming-job.html","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/tags/其他/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}]},{"title":"开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效","slug":"大数据/Spark/开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效","date":"2017-05-29T08:48:05.000Z","updated":"2018-11-29T11:56:29.496Z","comments":true,"path":"2017/05/29/大数据/Spark/开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效/","link":"","permalink":"http://yoursite.com/2017/05/29/大数据/Spark/开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效/","excerpt":"","text":"为了Spark Streaming应用能在生产中稳定、有效的执行，每批次数据处理时间（批处理时间）必须非常接近批次调度的时间间隔（批调度间隔），并且要一直低于批调度间隔。如果批处理时间一直高于批调度间隔，调度延迟就会一直增长并且不会恢复。最终，Spark Streaming应用会变得不再稳定。另一方面，如果批处理时间长时间远小于批调度间隔，就会浪费集群资源。 当Spark Streaming与Kafka使用Direct API集群时，我们可以很方便的去控制最大数据摄入量–通过一个被称作spark.streaming.kafka.maxRatePerPartition的参数。根据文档描述，他的含义是：Direct API读取每一个Kafka partition数据的最大速率（每秒读取的消息量）。 配置项spark.streaming.kafka.maxRatePerPartition，对防止流式应用在下边两种情况下出现流量过载时尤其重要： Kafka Topic中有大量未处理的消息，并且我们设置是Kafka auto.offset.reset参数值为smallest，他可以防止第一个批次出现数据流量过载情况。 当Kafka 生产者突然飙升流量的时候，他可以防止批次处理出现数据流量过载情况。 但是，配置Kafka每个partition每批次最大的摄入量是个静态值，也算是个缺点。随着时间的变化，在生产环境运行了一段时间的Spark Streaming应用，每批次每个Kafka partition摄入数据最大量的最优值也是变化的。有时候，是因为消息的大小会变，导致数据处理时间变化。有时候，是因为流计算所使用的多租户集群会变得非常繁忙，比如在白天时候，一些其他的数据应用（例如Impala/Hive/MR作业）竞争共享的系统资源时（CPU/内存/网络/磁盘IO）。 背压机制可以解决该问题。背压机制是呼声比较高的功能，他允许根据前一批次数据的处理情况，动态、自动的调整后续数据的摄入量，这样的反馈回路使得我们可以应对流式应用流量波动的问题。 Spark Streaming的背压机制是在Spark1.5版本引进的，我们可以添加如下代码启用改功能：1sparkConf.set(&quot;spark.streaming.backpressure.enabled&quot;,”true”) 那应用启动后的第一个批次流量怎么控制呢？因为他没有前面批次的数据处理时间，所以没有参考的数据去评估这一批次最优的摄入量。在Spark官方文档中有个被称作spark.streaming.backpressure.initialRate的配置，看起来是控制开启背压机制时初始化的摄入量。其实不然，该参数只对receiver模式起作用，并不适用于direct模式。推荐的方法是使用spark.streaming.kafka.maxRatePerPartition控制背压机制起作用前的第一批次数据的最大摄入量。我通常建议设置spark.streaming.kafka.maxRatePerPartition的值为最优估计值的1.5到2倍，让背压机制的算法去调整后续的值。请注意，spark.streaming.kafka.maxRatePerPartition的值会一直控制最大的摄入量，所以背压机制的算法值不会超过他。 另一个需要注意的是，在第一个批次处理完成前，紧接着的批次都将使用spark.streaming.kafka.maxRatePerPartition的值作为摄入量。通过Spark UI可以看到，批次间隔为5s，当批次调度延迟31秒时候，前7个批次的摄入量是20条记录。直到第八个批次，背压机制起作用时，摄入量变为5条记录。 翻译：http://www.linkedin.com/pulse/enable-back-pressure-make-your-spark-streaming-production-lan-jiang/","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"并行计算简介","slug":"编程基础/并行计算简介","date":"2017-04-02T08:48:05.000Z","updated":"2018-11-19T14:35:14.723Z","comments":true,"path":"2017/04/02/编程基础/并行计算简介/","link":"","permalink":"http://yoursite.com/2017/04/02/编程基础/并行计算简介/","excerpt":"","text":"什么是并行计算？串行计算 传统的软件通常被设计成为串行计算模式，具有如下特点： 一个问题被分解成为一系列离散的指令； 这些指令被顺次执行； 所有指令均在一个处理器上被执行； 在任何时刻，最多只有一个指令能够被执行。 比如： 并行计算 简单来讲，并行计算就是同时使用多个计算资源来解决一个计算问题： 一个问题被分解成为一系列可以并发执行的离散部分； 每个部分可以进一步被分解成为一系列离散指令； 来自每个部分的指令可以在不同的处理器上被同时执行； 需要一个总体的控制/协作机制来负责对不同部分的执行情况进行调度。 比如： 这里的 计算问题 需要具有如下特点： 能够被分解成为并发执行离散片段； 不同的离散片段能够被在任意时刻执行； 采用多个计算资源的花费时间要小于采用单个计算资源所花费的时间。 这里的 计算资源 通常包括： 具有多处理器/多核(multiple processors/cores)的计算机； 任意数量的被连接在一起的计算机。 为什么要并行计算？ 真实世界就是高度并行的： 自然界中的万事万物都在并发的，按照其内在时间序列运行着； 和串行计算相比，并行计算更适用于对现实世界中的复杂现象进行建模，模拟和理解； 例如，可以想象对这些进行顺序建模： 主要理由： 节约时间和成本： 理论上来讲，在一个任务上投入更多的资源有利于缩短其完成时间，从而降低成 并行计算机可以由大量廉价的单机构成，从而节约成本。 解决更大规模更复杂的问题： 很多问题的规模和复杂度使得其难以在一个单机上面完成； 一个有趣的例子：(Grand Challenge Problems)。 网页搜索引擎/数据库每秒处理百万级别的吞吐量。 提供并发性： 单个计算资源某个时间段只能做一件事情，而多计算资源则可以同时做多件事情； 协同网络可以使得来自世界不同地区的人同时虚拟地沟通。 利用非局部的资源： 可以利用更广范围中的网络资源； SETI@home的例子；以及 Folding@home的例子。 更好地利用并行硬件： 现代计算机甚至笔记本电脑在架构上都属于多处理器/多核的； 并行软件已经适用于多核的并行硬件条件，例如线程等； 在大多数情况下，运行在现代计算机上的串行程序实际上浪费了大量的计算资源。 概念和术语冯诺依曼体系结构以匈牙利数学家约翰·冯诺依曼命名的这一计算机体系结构，出现在他1945年发表的一篇论文中。这也通常被称为“存储程序计算机”——程序指令和数据都被保存在存储器中，这与早期通过“硬接线”编程的计算机不同。从此以后，所有的计算机遵从这一基本架构： 这里写图片描述 这里写图片描述 四个组成部分： 内存 控制器 处理器 输入输出 读写操作：支持随机存储的内存用来同时保存程序指令和数据： 程序指令用来指导计算机操作； 数据是程序用来操作的对象。 控制器：从内存中读取指令或者数据，对这些指令进行解码并且顺序执行这些指令。 处理器：提供基本的算术和逻辑操作。 输入输出设备：是人机交互的接口。 那么冯诺依曼体系结构和并行计算有什么关系呢？答案是：并行计算机仍然遵从这一基本架构，只是处理单元多于一个而已，其它的基本架构完全保持不变。 弗林的经典分类有不同的方法对并行计算机进行分类（具体例子可参见并行计算分类）。 一种被广泛采用的分类被称为弗林经典分类，诞生于1966年。弗林分类法从指令流和数据流两个维度区分多处理器计算机体系结构。每个维度有且仅有两个状态：单个或者多个。 下面个矩阵定义了弗林分类的四个可能状态： 单指令单数据(SISD)： SISD是标准意义上的串行机，具有如下特点：1）单指令：在每一个时钟周期内，CPU只能执行一个指令流；2）单数据：在每一个时钟周期内，输入设备只能输入一个数据流；3）执行结果是确定的。这是最古老的一种计算机类型。 单指令多数据(SIMD)： SIMD属于一种类型的并行计算机，具有如下特点：1）单指令：所有处理单元在任何一个时钟周期内都执行同一条指令；2）多数据：每个处理单元可以处理不同的数据元素；3）非常适合于处理高度有序的任务，例如图形/图像处理；4）同步（锁步）及确定性执行；5）两个主要类型：处理器阵列和矢量管道。多指令单数据(MISD)：MISD属于一种类型的并行计算机，具有如下特点：1）多指令：不同的处理单元可以独立地执行不同的指令流；2）单数据：不同的处理单元接收的是同一单数据流。这种架构理论上是有的，但是工业实践中这种机型非常少。 多指令多数据(MIMD)： MIMD属于最常见的一种类型的并行计算机，具有如下特点：1）多指令：不同的处理器可以在同一时刻处理不同的指令流；2）多数据：不同的处理器可以在同一时刻处理不同的数据；3）执行可以是同步的，也可以是异步的，可以是确定性的，也可以是不确定性的。这是目前主流的计算机架构类型，目前的超级计算机、并行计算机集群系统，网格，多处理器计算机，多核计算机等都属于这种类型。值得注意的是，许多MIMD类型的架构中实际也可能包括SIMD的子架构。 并行计算机的内存架构共享内存一般特征： 共享内存的并行计算机虽然也分很多种，但是通常而言，它们都可以让所有处理器以全局寻址的方式访问所有的内存空间。多个处理器可以独立地操作，但是它们共享同一片内存。一个处理器对内存地址的改变对其它处理器来说是可见的。根据内存访问时间，可以将已有的共享内存机器分为统一内存存取和非统一内存存取两种类型。 统一内存存取(Uniform Memory Access)： 目前更多地被称为对称多处理器机器(Symmetric Multiprocessor (SMP))，每个处理器都是相同的，并且其对内存的存取和存取之间都是无差别的。有时候也会被称为CC-UMA (Cache coherent - UMA)。缓存想干意味着如果一个处理器更新共享内存中的位置，则所有其它处理器都会了解该更新。缓存一致性是在硬件级别上实现的。 非统一内存存取(Non-Uniform Memory Access)： 通常由两个或者多个物理上相连的SMP。一个SMP可以存取其它SMP上的内存。不是所有处理器对所有内存都具有相同的存取或者存取时间。通过连接而进行内存存取速度会更慢一些。如果缓存相缓存想干的特性在这里仍然被保持，那么也可以被称为CC-NUMA。 优点：全局地址空间提供了一种用户友好的编程方式，并且由于内存与CPU的阶级程度，使得任务之间的数据共享既快速又统一。 缺点：最大的缺点是内存和CPU之间缺少较好的可扩展性。增加更多的CPU意味着更加共享内存和缓存想干系统上的存取流量，从而几何级别地增加缓存/内存管理的工作量。同时也增加了程序员的责任，因为他需要确保全局内存“正确”的访问以及同步。 分布式内存一般概念： 分布式内存架构也可以分为很多种，但是它们仍然有一些共同特征。分布式内存结构需要通讯网络，将不同的内存连接起来。一般而言，处理器会有它们所对应的内存。一个处理器所对应的内存地址不会映射到其它处理器上，所以在这种分布式内存架构中，不存在各个处理器所共享的全局内存地址。 由于每个处理器具有它所对应的局部内存，所以它们可以独立进行操作。一个本地内存上所发生的变化并不会被其它处理器所知晓。因此，缓存想干的概念在分布式内存架构中并不存在。 如果一个处理器需要对其它处理器上的数据进行存取，那么往往程序员需要明确地定义数据通讯的时间和方式，任务之间的同步因此就成为程序员的职责。尽管分布式内存架构中用于数据传输的网络结构可以像以太网一样简单，但在实践中它们的变化往往也很大。 优点： 1）内存可以随着处理器的数量而扩展，增加处理器的数量的同时，内存的大小也在成比例地增加；2）每个处理器可以快速地访问自己的内存而不会受到干扰，并且没有维护全局告诉缓存一致性所带来的开销；3）成本效益：可以使用现有的处理器和网络。 缺点： 1）程序员需要负责处理器之间数据通讯相关的许多细节；2）将基于全局内存的现有数据结构映射到该分布式内存组织可能会存在困难；3）非均匀的内存访问时间——驻留在远程结点上的数据比本地结点上的数据需要长的多的访问时间。 混合分布式-共享内存一般概念： 目前世界上最大和最快的并行计算机往往同时具有分布式和共享式的内存架构。共享式内存架构可以是共线内存机器或者图形处理单元(GPU)。分布式内存组件可以是由多个共享内存/GPU连接而成的系统。每个结点只知道自己的内存，不知道网络上其它结点的内存。因此，需要在不同的机器上通过网络进行数据通讯。 从目前的趋势来看，这种混合式的内存架构将长期占有主导地位，并且成为高端计算在可见的未来中的最好选择。 优缺点： 1）继承了共享式内存和分布式内存的优缺点；2）优点之一是可扩展性；3）缺点之一是编程的复杂性。 翻译：https://computing.llnl.gov/tutorials/parallel_comp/","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/tags/其他/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}]},{"title":"深入浅出Parquet Schema","slug":"大数据/Spark/Parquet简介","date":"2016-07-29T08:48:05.000Z","updated":"2018-12-10T04:14:49.463Z","comments":true,"path":"2016/07/29/大数据/Spark/Parquet简介/","link":"","permalink":"http://yoursite.com/2016/07/29/大数据/Spark/Parquet简介/","excerpt":"","text":"背景 相信使用Spark比较多的人，对Parquet都不陌生。 其是Dremel的开源实现，作为一种列式存储文件格式，2015年称为 Apache 顶级项目，后来被 Spark 项目吸收，作为 Spark 的默认数据源，在不指定读取和存储格式时，默认读写 Parquet 格式的文件。 2010年 google 发表了一篇论文《Dremel: Interactive Analysis of Web-Scale Datasets》https://ai.google/research/pubs/pub36632 A key strength of Parquet is its ability to store data that has a deeply nested structure in true columnar fashion.The result is that even nested fields can be read independently of other fields, resulting in significant performance improvements. Another feature of Parquet is the large number of tools that support it as a format.MapReduce, Pig, Hive, Cascading, Crunch, and Spark ，介绍了其 Dermel 系统是如何利用列式存储管理嵌套数据的，嵌套数据就是层次数据，如定义一个班级，班级由同学组成，同学的信息有学号、年龄、身高等。 今天不介绍嵌套数据是如何映射到每一列了，简单来说就是把不同层级的属性拍到一级，类似降维打击。这样，一个嵌套数据可以看成独立的多个属性，每一个属性就是一列，和表结构差不多。 写流程虽然是按列存储，但数据是一行一行来的，那什么时候将内存中的数据写文件呢？我们知道文件只能顺序写，假如每收到一行数据就写入磁盘，那就是行式存储了。 一个解决方案是为每个列开一个文件，假如数据有 n 个属性，就需要 n 个文件，每次写数据就需要追加到 n 个文件中。但是对于文件格式来说，用户肯定希望把复杂的数据存到一个文件中，而不希望管理一堆小文件（可以想象你做了一个ppt，每一页存成了一个文件），所以一个 Parquet 文件中必须存储数据的所有属性。 另一个解决方案是在内存中缓存一些数据，等缓存到一定量后，将各个列的数据放在一起打包，这样各个包就可以按一定顺序写到一个文件中。这就是列式存储的精髓：按列缓存打包。 文件格式按照上边这种方式，Parquet 在每一列内也需要分成一个个的数据包，这个数据包就叫 Page，Page 的分割标准可以按数据点数（如每1000行数据打成一个 Page），也可以按空间占用（如每列的数据攒到8KB合成一个 Page）。 一个 Page 的数据就是一列，类型相同，在存储到磁盘之前一般都会进行编码压缩，为了快速查询、也为了解压缩这一个 Page，在写的时候先统计一下最大最小值，叫做 PageHeader，存储在 Page 的开头，其实就是 Page 的 元数据（metadata）。PageHeader 后边就是数据了，读取一个 Page 时，可以先通过 PageHeader 进行过滤。 Parquet 又把多个 Page 放在一起存储，叫 Column Chunk。于是，每一列都由多个 Column Chunk 组成，并且也有其对应的 ColumnChunk Metadata。注意，这只是一个完整数据的一个属性，一个数据的多个属性要放在多个 Column Chunk 的，这多个 Column Chunk 放在一起就叫做一个 Row Group。 下边这就是 Parquet 官方介绍：- Parquet文件在磁盘上的分布情况如图5所示。所有的数据被水平切分成Rowgroup，一个Row group包含这个Rowgroup对应的区间内的所有列的column chunk。一个columnchunk负责存储某一列的数据，这些数据是这一列的Repetition levels, Definition levels和values（详见后文）。一个columnchunk是由Page组成的，Page是压缩和编码的单元，对数据模型来说是透明的。一个Parquet文件最后是Footer，存储了文件的元数据信息和统计信息。Row group是数据读写时候的缓存单元，所以推荐设置较大的Rowgroup从而带来较大的并行度，当然也需要较大的内存空间作为代价。一般情况下推荐配置一个Row group大小1G，一个HDFS块大小1G，一个HDFS文件只含有一个块[s2]。 protobuffGoogle Protocol Buffer( 简称 Protobuf) 是 Google 公司内部的混合语言数据标准。Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API。 我们可以考察 Protobuf 序列化后的信息内容。您可以看到 Protocol Buffer 信息的表示非常紧凑，这意味着消息的体积减少，自然需要更少的资源。比如网络上传输的字节数更少，需要的 IO 更少等，从而提高性能。 由于文本并不适合用来描述数据结构，所以 Protobuf 也不适合用来对基于文本的标记文档（如 HTML）建模。另外，由于 XML 具有某种程度上的自解释性，它可以被人直接读取编辑，在这一点上 Protobuf 不行，它以二进制的方式存储，除非你有 .proto 定义，否则你没法直接读出 Protobuf 的任何内容 列式存储： 按列存，能够更好地压缩数据，因为一列的数据一般都是同质的（homogenous）. 对于hadoop集群来说，空间节省非常可观. I/O 会大大减少，因为扫描（遍历/scan）的时候，可以只读其中部分列. 而且由于数据压缩的更好的缘故，IO所需带宽也会减小. 由于每列存的数据类型是相同的，we can use encodings better suited to the modern processors’ pipeline by making instruction branching more predictable. （没想好怎么翻译，各位自己理解吧） https://www.cnblogs.com/ulysses-you/p/7985240.htmlhttps://blog.csdn.net/macyang/article/details/8566105","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/tags/其他/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}]},{"title":"Spark SQL功能测试及总结(1.4.1版本)","slug":"大数据/Spark/Spark SQL功能测试及总结(1.4.1版本)","date":"2016-06-11T08:48:05.000Z","updated":"2018-09-12T07:30:28.611Z","comments":true,"path":"2016/06/11/大数据/Spark/Spark SQL功能测试及总结(1.4.1版本)/","link":"","permalink":"http://yoursite.com/2016/06/11/大数据/Spark/Spark SQL功能测试及总结(1.4.1版本)/","excerpt":"","text":"背景spark sql在项目中使用越来越多，spark sql都支持哪些功能？官网没有明确说明，只能在class SqlParserd代码中看到一些Keyword，所以准备测试下spark对常用sql的支持情况。 一、测试总结12345678910111213141516171、结果默认正向排序2、distinct 去重：大小写敏感，可作用于其后多个字段3、limit 限制返回结果条数：前n条4、排序 order by：作用于其后多列(可分别指定排序方式)；排序字段可为非选择列；默认正续；不支持列位置排序(不抛异常，按第一列正续)5、支持的where子句操作符：=,&lt;&gt;,!=,&lt;,&lt;=,&gt;,&gt;=,between,is null,is not null6、组合where子句：and、or、in、not，and 优先级高于 or；not 可修饰in；为表达方便，or与in建议优先选择in7、模糊查找like通配符：%，- ，通配符对数据类型也起作用8、支持表和字段别名9、不支持字段拼接10、支持算术计算：+,-,*,/,%11、字符串函数：lower、upper，没去空格函数12、数值函数 ：abs、sqrt，没三角函数13、聚集函数：avg、count、sum、max、min，avg忽略为空的行，作用于字符串返回null；count 不忽略空行；max、min也可以作用于字符串列，分别返回第一行最后一行；avg，max，min 不支持去重后求值14、支持分组 group by及分组过滤having15、支持表内联结、左外联结、右外联结、全联结16、组合union：union结果去重，union all不去重(功能同unionAll函数)17、不支持子查询(2.0版本支持：https://issues.apache.org/jira/browse/SPARK-4226) 二、测试数据表1：Products1234567891011121314+-------+-------+-------------------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+-------------------+----------+--------------------+| BR01| BRS01| 8 inch teddy bear| 5.99| 8 inch teddy bear|| BR02| BRS01| 12 inch teddy bear| 8.99| 12 inch teddy bear|| BR03| BRS01| 18 inch teddy bear| 11.99| 18 inch teddy bear|| BNBG01| DLL01| Fish bean bag toy| 3.49| Fish bean bag toy|| BNBG02| DLL01| Bird bean bag toy| 3.49| Bird bean bag toy|| BNBG03| DLL01|Rabbit bean bag toy| 3.49| Rabbit bean bag toy|| RGAN01| DLL01| Raggedy Ann| 4.99|18 inch Raggedy A...|| RYL01| FNG01| King doll| 9.49|12 inch king doll...|| RYL02| FNG01| Queen doll| 9.49|12 inch queen dol...|| ryl02| fng01| Queen doll| 0.0|12 inch queen dol...|+-------+-------+-------------------+----------+--------------------+ 表2：Vendors12345678910+-------+---------------+---------------+----------+----------+--------+------------+|vend_id| vend_name| vend_address| vend_city|vend_state|vend_zip|vend_country|+-------+---------------+---------------+----------+----------+--------+------------+| BRS01| Bears R Us|123 Main Street| Bear Town| MI| 44444| USA|| BRE02| Bear Emporium|500 Park Street| Anytown| OH| 44333| USA|| DLL01|Doll House Inc.|555 High Street|Dollsville| CA| 99999| USA|| FRB01| Furball Inc.|1000 5th Avenue| New York| NY| 11111| USA|| FNG01| Fun and Games| 42 Galaxy Road| London| NULL| N16 6PS| England|| JTS01| Jouets et ours|1 Rue Amusement| Paris| NULL| 45678| France|+-------+---------------+---------------+----------+----------+--------+------------+ 三、测试sql及结果1、数据检索1）检索列123456789101112131415161718192021222324252627282930313233343536373839404142434445ssc.sql(&quot;select prod_name from Products&quot;).show+-------------------+| prod_name|+-------------------+| 8 inch teddy bear|| 12 inch teddy bear|| 18 inch teddy bear|| Fish bean bag toy|| Bird bean bag toy||Rabbit bean bag toy|| Raggedy Ann|| King doll|| Queen doll|| Queen doll|+-------------------+ssc.sql(&quot;select prod_id, prod_name,prod_price from Products&quot;).show+-------+-------------------+----------+|prod_id| prod_name|prod_price|+-------+-------------------+----------+| BR01| 8 inch teddy bear| 5.99|| BR02| 12 inch teddy bear| 8.99|| BR03| 18 inch teddy bear| 11.99|| BNBG01| Fish bean bag toy| 3.49|| BNBG02| Bird bean bag toy| 3.49|| BNBG03|Rabbit bean bag toy| 3.49|| RGAN01| Raggedy Ann| 4.99|| RYL01| King doll| 9.49|| RYL02| Queen doll| 9.49|| ryl02| Queen doll| 0.0|+-------+-------------------+----------+select * from Products+-------+-------+-------------------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+-------------------+----------+--------------------+| BR01| BRS01| 8 inch teddy bear| 5.99| 8 inch teddy bear|| BR02| BRS01| 12 inch teddy bear| 8.99| 12 inch teddy bear|| BR03| BRS01| 18 inch teddy bear| 11.99| 18 inch teddy bear|| BNBG01| DLL01| Fish bean bag toy| 3.49| Fish bean bag toy|| BNBG02| DLL01| Bird bean bag toy| 3.49| Bird bean bag toy|| BNBG03| DLL01|Rabbit bean bag toy| 3.49| Rabbit bean bag toy|| RGAN01| DLL01| Raggedy Ann| 4.99|18 inch Raggedy A...|| RYL01| FNG01| King doll| 9.49|12 inch king doll...|| RYL02| FNG01| Queen doll| 9.49|12 inch queen dol...|| ryl02| fng01| Queen doll| 0.0|12 inch queen dol...|+-------+-------+-------------------+----------+--------------------+ 说明：返回结果默认按第一列正向排序2）检索不同的值 distinct123456789ssc.sql(&quot;select distinct vend_id from Products&quot;).show+-------+|vend_id|+-------+| DLL01|| FNG01|| BRS01|| fng01|+-------+ 说明：大小写敏感；可作用于其后多列 3）限制结果条数 limit12345678910ssc.sql(&quot;select prod_name from Products limit 5&quot;)+------------------+| prod_name|+------------------+| 8 inch teddy bear||12 inch teddy bear||18 inch teddy bear|| Fish bean bag toy|| Bird bean bag toy|+------------------+ 说明：取前n条4）数据排序 order by123456789101112131415161718192021222324252627282930ssc.sql(&quot;select prod_id, prod_name from Products order by prod_id desc,prod_price&quot;).show+-------+-------------------+|prod_id| prod_name|+-------+-------------------+| ryl02| Queen doll|| RYL02| Queen doll|| RYL01| King doll|| RGAN01| Raggedy Ann|| BR03| 18 inch teddy bear|| BR02| 12 inch teddy bear|| BR01| 8 inch teddy bear|| BNBG03|Rabbit bean bag toy|| BNBG02| Bird bean bag toy|| BNBG01| Fish bean bag toy|+-------+-------------------+ssc.sql(&quot;select prod_id, prod_name from Products order by 0 desc,22,prod_price&quot;).show+-------+-------------------+|prod_id| prod_name|+-------+-------------------+| BR01| 8 inch teddy bear|| BR02| 12 inch teddy bear|| BR03| 18 inch teddy bear|| BNBG01| Fish bean bag toy|| BNBG02| Bird bean bag toy|| BNBG03|Rabbit bean bag toy|| RGAN01| Raggedy Ann|| RYL01| King doll|| RYL02| Queen doll|| ryl02| Queen doll|+-------+-------------------+ 说明：order by要作为select的最后一个子句；作用于其后多列(可分别指定排序方式)；排序字段可为非选择列；默认正续；不支持列位置排序(不抛异常，按第一列正续)二、数据过滤1）where 子句操作符：=,&lt;&gt;,!=,&lt;,&lt;=,&gt;,&gt;=,between,is null,is not null；12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970select * from Products where prod_price = 3.49+-------+-------+-------------------+----------+-------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+-------------------+----------+-------------------+| BNBG01| DLL01| Fish bean bag toy| 3.49| Fish bean bag toy|| BNBG02| DLL01| Bird bean bag toy| 3.49| Bird bean bag toy|| BNBG03| DLL01|Rabbit bean bag toy| 3.49|Rabbit bean bag toy|+-------+-------+-------------------+----------+-------------------+ssc.sql(&quot;select * from Products where prod_price != 3.49&quot;).show+-------+-------+------------------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+------------------+----------+--------------------+| BR01| BRS01| 8 inch teddy bear| 5.99| 8 inch teddy bear|| BR02| BRS01|12 inch teddy bear| 8.99| 12 inch teddy bear|| BR03| BRS01|18 inch teddy bear| 11.99| 18 inch teddy bear|| RGAN01| DLL01| Raggedy Ann| 4.99|18 inch Raggedy A...|| RYL01| FNG01| King doll| 9.49|12 inch king doll...|| RYL02| FNG01| Queen doll| 9.49|12 inch queen dol...|| ryl02| fng01| Queen doll| 0.0|12 inch queen dol...|+-------+-------+------------------+----------+--------------------+ssc.sql(&quot;select * from Products where prod_price &lt;= 5&quot;).show+-------+-------+-------------------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+-------------------+----------+--------------------+| BNBG01| DLL01| Fish bean bag toy| 3.49| Fish bean bag toy|| BNBG02| DLL01| Bird bean bag toy| 3.49| Bird bean bag toy|| BNBG03| DLL01|Rabbit bean bag toy| 3.49| Rabbit bean bag toy|| RGAN01| DLL01| Raggedy Ann| 4.99|18 inch Raggedy A...|| ryl02| fng01| Queen doll| 0.0|12 inch queen dol...|+-------+-------+-------------------+----------+--------------------+ssc.sql(&quot;select * from Products where vend_id != &apos;DLL01&apos;&quot;).show+-------+-------+------------------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+------------------+----------+--------------------+| BR01| BRS01| 8 inch teddy bear| 5.99| 8 inch teddy bear|| BR02| BRS01|12 inch teddy bear| 8.99| 12 inch teddy bear|| BR03| BRS01|18 inch teddy bear| 11.99| 18 inch teddy bear|| RYL01| FNG01| King doll| 9.49|12 inch king doll...|| RYL02| FNG01| Queen doll| 9.49|12 inch queen dol...|| ryl02| fng01| Queen doll| 0.0|12 inch queen dol...|+-------+-------+------------------+----------+--------------------+ssc.sql(&quot;select * from Products where prod_price between 5 and 10&quot;).show+-------+-------+------------------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+------------------+----------+--------------------+| BR01| BRS01| 8 inch teddy bear| 5.99| 8 inch teddy bear|| BR02| BRS01|12 inch teddy bear| 8.99| 12 inch teddy bear|| RYL01| FNG01| King doll| 9.49|12 inch king doll...|| RYL02| FNG01| Queen doll| 9.49|12 inch queen dol...|+-------+-------+------------------+----------+--------------------+ssc.sql(&quot;select * from Products where prod_price is null &quot;).show+-------+-------+---------+----------+---------+|prod_id|vend_id|prod_name|prod_price|prod_desc|+-------+-------+---------+----------+---------++-------+-------+---------+----------+---------+ssc.sql(&quot;select * from Products where prod_price is not null &quot;).show+-------+-------+-------------------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+-------------------+----------+--------------------+| BR01| BRS01| 8 inch teddy bear| 5.99| 8 inch teddy bear|| BR02| BRS01| 12 inch teddy bear| 8.99| 12 inch teddy bear|| BR03| BRS01| 18 inch teddy bear| 11.99| 18 inch teddy bear|| BNBG01| DLL01| Fish bean bag toy| 3.49| Fish bean bag toy|| BNBG02| DLL01| Bird bean bag toy| 3.49| Bird bean bag toy|| BNBG03| DLL01|Rabbit bean bag toy| 3.49| Rabbit bean bag toy|| RGAN01| DLL01| Raggedy Ann| 4.99|18 inch Raggedy A...|| RYL01| FNG01| King doll| 9.49|12 inch king doll...|| RYL02| FNG01| Queen doll| 9.49|12 inch queen dol...|| ryl02| fng01| Queen doll| 0.0|12 inch queen dol...|+-------+-------+-------------------+----------+--------------------+ 2）组合where子句123456789101112131415161718192021222324252627282930313233343536373839404142ssc.sql(&quot;select * from Products where vend_id = &apos;DLL01&apos; or vend_id = &apos;BRS01&apos;&quot;).show+-------+-------+-------------------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+-------------------+----------+--------------------+| BR01| BRS01| 8 inch teddy bear| 5.99| 8 inch teddy bear|| BR02| BRS01| 12 inch teddy bear| 8.99| 12 inch teddy bear|| BR03| BRS01| 18 inch teddy bear| 11.99| 18 inch teddy bear|| BNBG01| DLL01| Fish bean bag toy| 3.49| Fish bean bag toy|| BNBG02| DLL01| Bird bean bag toy| 3.49| Bird bean bag toy|| BNBG03| DLL01|Rabbit bean bag toy| 3.49| Rabbit bean bag toy|| RGAN01| DLL01| Raggedy Ann| 4.99|18 inch Raggedy A...|+-------+-------+-------------------+----------+--------------------+ssc.sql(&quot;select * from Products where vend_id = &apos;DLL01&apos; or vend_id = &apos;BRS01&apos; and prod_price &gt; 10&quot;).show+-------+-------+-------------------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+-------------------+----------+--------------------+| BR03| BRS01| 18 inch teddy bear| 11.99| 18 inch teddy bear|| BNBG01| DLL01| Fish bean bag toy| 3.49| Fish bean bag toy|| BNBG02| DLL01| Bird bean bag toy| 3.49| Bird bean bag toy|| BNBG03| DLL01|Rabbit bean bag toy| 3.49| Rabbit bean bag toy|| RGAN01| DLL01| Raggedy Ann| 4.99|18 inch Raggedy A...|+-------+-------+-------------------+----------+--------------------+ssc.sql(&quot;select * from Products where (vend_id = &apos;DLL01&apos; or vend_id = &apos;BRS01&apos;) and prod_price &gt; 10&quot;).show+-------+-------+------------------+----------+------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+------------------+----------+------------------+| BR03| BRS01|18 inch teddy bear| 11.99|18 inch teddy bear|+-------+-------+------------------+----------+------------------+ssc.sql(&quot;select * from Products where vend_id in (&apos;DLL01&apos;,&apos;BRS01&apos;) and prod_price &gt; 10&quot;).show+-------+-------+------------------+----------+------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+------------------+----------+------------------+| BR03| BRS01|18 inch teddy bear| 11.99|18 inch teddy bear|+-------+-------+------------------+----------+------------------+ssc.sql(&quot;select * from Products where vend_id not in (&apos;DLL01&apos;,&apos;BRS01&apos;)&quot;).show+-------+-------+----------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+----------+----------+--------------------+| RYL01| FNG01| King doll| 9.49|12 inch king doll...|| RYL02| FNG01|Queen doll| 9.49|12 inch queen dol...|| ryl02| fng01|Queen doll| 0.0|12 inch queen dol...|+-------+-------+----------+----------+--------------------+ 总结：and 优先级高于 or；not 可修饰in；为表达方便，or与in建议优先选择in 3）通配符1234567891011121314151617181920212223242526ssc.sql(&quot;select * from Products where prod_name like &apos;%doll%&apos;&quot;).show+-------+-------+----------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+----------+----------+--------------------+| RYL01| FNG01| King doll| 9.49|12 inch king doll...|| RYL02| FNG01|Queen doll| 9.49|12 inch queen dol...|| ryl02| fng01|Queen doll| 0.0|12 inch queen dol...|+-------+-------+----------+----------+--------------------+ssc.sql(&quot;select * from Products where prod_price like &apos;%4%&apos;&quot;).show+-------+-------+-------------------+----------+--------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+-------------------+----------+--------------------+| BNBG01| DLL01| Fish bean bag toy| 3.49| Fish bean bag toy|| BNBG02| DLL01| Bird bean bag toy| 3.49| Bird bean bag toy|| BNBG03| DLL01|Rabbit bean bag toy| 3.49| Rabbit bean bag toy|| RGAN01| DLL01| Raggedy Ann| 4.99|18 inch Raggedy A...|| RYL01| FNG01| King doll| 9.49|12 inch king doll...|| RYL02| FNG01| Queen doll| 9.49|12 inch queen dol...|+-------+-------+-------------------+----------+--------------------+ssc.sql(&quot;select * from Products where prod_name like &apos;__ inch teddy bear&apos;&quot;+-------+-------+------------------+----------+------------------+|prod_id|vend_id| prod_name|prod_price| prod_desc|+-------+-------+------------------+----------+------------------+| BR02| BRS01|12 inch teddy bear| 8.99|12 inch teddy bear|| BR03| BRS01|18 inch teddy bear| 11.99|18 inch teddy bear|+-------+-------+------------------+----------+------------------+ 4）不支持字段拼接123456789101112131415ssc.sql(&quot;select prod_name + &apos;-&apos; + prod_id from Products&quot;).show+----+| c0|+----+|null||null||null||null||null||null||null||null||null||null|+----+ 5）字段算数运算 (+,-,*,/,%)123456789101112131415ssc.sql(&quot;select prod_name ,prod_price,prod_price*2 from Products&quot;).show+-------------------+----------+-----+| prod_name|prod_price| c2|+-------------------+----------+-----+| 8 inch teddy bear| 5.99|11.98|| 12 inch teddy bear| 8.99|17.98|| 18 inch teddy bear| 11.99|23.98|| Fish bean bag toy| 3.49| 6.98|| Bird bean bag toy| 3.49| 6.98||Rabbit bean bag toy| 3.49| 6.98|| Raggedy Ann| 4.99| 9.98|| King doll| 9.49|18.98|| Queen doll| 9.49|18.98|| Queen doll| 0.0| 0.0|+-------------------+----------+-----+ 6）字符串函数（不支持去空格）123456789101112131415ssc.sql(&quot;select lower(prod_name),upper(prod_name),prod_price from Products&quot;).show+-------------------+-------------------+----------+| c0| c1|prod_price|+-------------------+-------------------+----------+| 8 inch teddy bear| 8 INCH TEDDY BEAR| 5.99|| 12 inch teddy bear| 12 INCH TEDDY BEAR| 8.99|| 18 inch teddy bear| 18 INCH TEDDY BEAR| 11.99|| fish bean bag toy| FISH BEAN BAG TOY| 3.49|| bird bean bag toy| BIRD BEAN BAG TOY| 3.49||rabbit bean bag toy|RABBIT BEAN BAG TOY| 3.49|| raggedy ann| RAGGEDY ANN| 4.99|| king doll| KING DOLL| 9.49|| queen doll| QUEEN DOLL| 9.49|| queen doll| QUEEN DOLL| 0.0|+-------------------+-------------------+----------+ 7）数值函数123456789101112131415ssc.sql(&quot;select prod_price, abs(prod_price), sqrt(prod_price) from Products&quot;).show+----------+-----+------------------+|prod_price| c1| c2|+----------+-----+------------------+| 5.99| 5.99|2.4474476501040834|| 8.99| 8.99| 2.99833287011299|| 11.99|11.99| 3.462657938636157|| 3.49| 3.49|1.8681541692269406|| 3.49| 3.49|1.8681541692269406|| 3.49| 3.49|1.8681541692269406|| 4.99| 4.99| 2.233830790368868|| 9.49| 9.49|3.0805843601498726|| 9.49| 9.49|3.0805843601498726|| 0.0| 0.0| 0.0|+----------+-----+------------------+ 8）聚集函数123456789101112ssc.sql(&quot;select avg(prod_price) avg_price from Products&quot;).show+-----------------+| avg_price|+-----------------+|6.141000000000001|+-----------------+ssc.sql(&quot;select count(prod_price) c from Products&quot;).show+--+| c|+--+|10|+--+ 说明：avg忽略为空的行，作用于字符串返回null；count 不忽略空行123456ssc.sql(&quot;select max(prod_price) m from Products&quot;).show+-----+| m|+-----+|11.99|+-----+ 说明：同min也可以作用于字符串列，分别返回第一行最后一行123456789101112ssc.sql(&quot;select sum(prod_name) s from Products&quot;).show+---+| s|+---+|0.0|+---+ssc.sql(&quot;select sum(distinct prod_price),count(distinct prod_price)from Products&quot;).show+------------------+--+| c0|c1|+------------------+--+|44.940000000000005| 7|+------------------+--+ 说明：avg，max，min 不支持去重后求值9）分组 group by12345678910111213141516171819202122ssc.sql(&quot;select vend_id ,count(*),max(prod_price),avg(prod_price) from Products group by vend_id&quot;).show+-------+--+-----+-----+|vend_id|c1| c2| c3|+-------+--+-----+-----+| DLL01| 4| 4.99|3.865|| FNG01| 2| 9.49| 9.49|| BRS01| 3|11.99| 8.99|| fng01| 1| 0.0| 0.0|+-------+--+-----+-----+ssc.sql(&quot;select vend_id ,count(*) c,max(prod_price) m,avg(prod_price) from Products where prod_price &gt; 0 group by vend_id having m &gt; 5 and c &gt; 2&quot;).show+-------+-+-----+----+|vend_id|c| m| c3|+-------+-+-----+----+| BRS01|3|11.99|8.99|+-------+-+-----+----+ssc.sql(&quot;select vend_id ,count(*) c,max(prod_price) m,avg(prod_price) from Products where prod_price &gt; 0 group by vend_id having m &gt; 5 order by c3&quot;).show+-------+-+-----+----+|vend_id|c| m| c3|+-------+-+-----+----+| BRS01|3|11.99|8.99|| FNG01|2| 9.49|9.49|+-------+-+-----+----+ 10）过滤分组12345678ssc.sql(&quot;select vend_id from Products group by vend_id having avg(prod_price) &gt; 3&quot;).show+-------+|vend_id|+-------+| DLL01|| FNG01|| BRS01|+-------+ 11）表联结1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374ssc.sql(&quot;select vend_name,prod_name,prod_price from Vendors v,Products p where v.vend_id = p.vend_id &quot;).showssc.sql(&quot;select vend_name,prod_name,prod_price from Vendors v inner join Products p on v.vend_id = p.vend_id &quot;).show+---------------+-------------------+----------+| vend_name| prod_name|prod_price|+---------------+-------------------+----------+|Doll House Inc.| Fish bean bag toy| 3.49||Doll House Inc.| Bird bean bag toy| 3.49||Doll House Inc.|Rabbit bean bag toy| 3.49||Doll House Inc.| Raggedy Ann| 4.99|| Fun and Games| King doll| 9.49|| Fun and Games| Queen doll| 9.49|| Bears R Us| 8 inch teddy bear| 5.99|| Bears R Us| 12 inch teddy bear| 8.99|| Bears R Us| 18 inch teddy bear| 11.99|+---------------+-------------------+----------+ssc.sql(&quot;select vend_name,prod_name,prod_price from Vendors v left join Products p on v.vend_id = p.vend_id &quot;).show+---------------+-------------------+----------+| vend_name| prod_name|prod_price|+---------------+-------------------+----------+| Bear Emporium| null| null|| Furball Inc.| null| null|| Jouets et ours| null| null||Doll House Inc.| Fish bean bag toy| 3.49||Doll House Inc.| Bird bean bag toy| 3.49||Doll House Inc.|Rabbit bean bag toy| 3.49||Doll House Inc.| Raggedy Ann| 4.99|| Fun and Games| King doll| 9.49|| Fun and Games| Queen doll| 9.49|| Bears R Us| 8 inch teddy bear| 5.99|| Bears R Us| 12 inch teddy bear| 8.99|| Bears R Us| 18 inch teddy bear| 11.99|+---------------+-------------------+----------+ssc.sql(&quot;select vend_name,prod_name,prod_price from Vendors v right outer join Products p on v.vend_id = p.vend_id &quot;).show+---------------+-------------------+----------+| vend_name| prod_name|prod_price|+---------------+-------------------+----------+|Doll House Inc.| Fish bean bag toy| 3.49||Doll House Inc.| Bird bean bag toy| 3.49||Doll House Inc.|Rabbit bean bag toy| 3.49||Doll House Inc.| Raggedy Ann| 4.99|| Fun and Games| King doll| 9.49|| Fun and Games| Queen doll| 9.49|| Bears R Us| 8 inch teddy bear| 5.99|| Bears R Us| 12 inch teddy bear| 8.99|| Bears R Us| 18 inch teddy bear| 11.99|| null| Queen doll| 0.0|+---------------+-------------------+----------+ssc.sql(&quot;select vend_name,prod_name,prod_price from Vendors v full outer join Products p on v.vend_id = p.vend_id &quot;).show+---------------+-------------------+----------+| vend_name| prod_name|prod_price|+---------------+-------------------+----------+| Furball Inc.| null| null|| Bear Emporium| null| null|| Jouets et ours| null| null||Doll House Inc.| Fish bean bag toy| 3.49||Doll House Inc.| Bird bean bag toy| 3.49||Doll House Inc.|Rabbit bean bag toy| 3.49||Doll House Inc.| Raggedy Ann| 4.99|| Fun and Games| King doll| 9.49|| Fun and Games| Queen doll| 9.49|| Bears R Us| 8 inch teddy bear| 5.99|| Bears R Us| 12 inch teddy bear| 8.99|| Bears R Us| 18 inch teddy bear| 11.99|| null| Queen doll| 0.0|+---------------+-------------------+----------+ssc.sql(&quot;select vend_name ,count(prod_id) c from Products p left outer join Vendors v on v.vend_id = p.vend_id group by vend_name&quot;).show+---------------+-+| vend_name|c|+---------------+-+| Fun and Games|2||Doll House Inc.|4|| Bears R Us|3|| null|1|+---------------+-+ 12）组合查询1234567891011121314151617181920212223242526ssc.sql(&quot;select * from Vendors v where vend_id = &apos;JTS01&apos; or vend_state= &apos;MI&apos;&quot;).unionAll(ssc.sql(&quot;select * from Vendors v where v.vend_state in (&apos;MI&apos;,&apos;OH&apos;)&quot;)).show+-------+--------------+---------------+---------+----------+--------+------------+|vend_id| vend_name| vend_address|vend_city|vend_state|vend_zip|vend_country|+-------+--------------+---------------+---------+----------+--------+------------+| BRS01| Bears R Us|123 Main Street|Bear Town| MI| 44444| USA|| JTS01|Jouets et ours|1 Rue Amusement| Paris| NULL| 45678| France|| BRS01| Bears R Us|123 Main Street|Bear Town| MI| 44444| USA|| BRE02| Bear Emporium|500 Park Street| Anytown| OH| 44333| USA|+-------+--------------+---------------+---------+----------+--------+------------+ssc.sql(&quot;select * from Vendors v where vend_id = &apos;JTS01&apos; or vend_state= &apos;MI&apos; union select * from Vendors v where v.vend_state in (&apos;MI&apos;,&apos;OH&apos;)&quot;).show+-------+--------------+---------------+---------+----------+--------+------------+|vend_id| vend_name| vend_address|vend_city|vend_state|vend_zip|vend_country|+-------+--------------+---------------+---------+----------+--------+------------+| BRS01| Bears R Us|123 Main Street|Bear Town| MI| 44444| USA|| JTS01|Jouets et ours|1 Rue Amusement| Paris| NULL| 45678| France|| BRE02| Bear Emporium|500 Park Street| Anytown| OH| 44333| USA|+-------+--------------+---------------+---------+----------+--------+------------+ssc.sql(&quot;select * from Vendors v where vend_id = &apos;JTS01&apos; or vend_state= &apos;MI&apos; union all select * from Vendors v where v.vend_state in (&apos;MI&apos;,&apos;OH&apos;)&quot;).show+-------+--------------+---------------+---------+----------+--------+------------+|vend_id| vend_name| vend_address|vend_city|vend_state|vend_zip|vend_country|+-------+--------------+---------------+---------+----------+--------+------------+| BRS01| Bears R Us|123 Main Street|Bear Town| MI| 44444| USA|| JTS01|Jouets et ours|1 Rue Amusement| Paris| NULL| 45678| France|| BRS01| Bears R Us|123 Main Street|Bear Town| MI| 44444| USA|| BRE02| Bear Emporium|500 Park Street| Anytown| OH| 44333| USA|+-------+--------------+---------------+---------+----------+--------+------------+ 说明：union 排重四、代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139import org.apache.spark.sql.types._import org.apache.spark.sql.&#123;Row, SQLContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by xuemin.zhang on 16/6/12. */object SqlTestOnParquet &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;test&quot;) val sc = new SparkContext(conf) val ssc = new SQLContext(sc) val products_data = Array( &quot;BR01,BRS01,8 inch teddy bear,5.99,8 inch teddy bear,comes with cap and jacket&quot;, &quot;BR02,BRS01,12 inch teddy bear,8.99,12 inch teddy bear,comes with cap and jacket&quot;, &quot;BR03,BRS01,18 inch teddy bear,11.99,18 inch teddy bear,comes with cap and jacket&quot;, &quot;BNBG01,DLL01,Fish bean bag toy,3.49,Fish bean bag toy,complete with bean bag worms with which to feed it&quot;, &quot;BNBG02,DLL01,Bird bean bag toy,3.49,Bird bean bag toy,eggs are not included&quot;, &quot;BNBG03,DLL01,Rabbit bean bag toy,3.49,Rabbit bean bag toy,comes with bean bag carrots&quot;, &quot;RGAN01,DLL01,Raggedy Ann,4.990,18 inch Raggedy Ann doll&quot;, &quot;RYL01,FNG01,King doll,9.49,12 inch king doll with royal garments and crown&quot;, &quot;RYL02,FNG01,Queen doll,9.49,12 inch queen doll with royal garments and crown&quot;, &quot;ryl02,fng01,Queen doll,0,12 inch queen doll with royal garments and crown&quot; ) val productsRdd = sc.parallelize(products_data).map(_.split(&quot;,&quot;)).map(p =&gt; Row(p(0), p(1), p(2), p(3).toDouble, p(4))) val productsFields = Array( StructField(&quot;prod_id&quot;,StringType,true), StructField(&quot;vend_id&quot;,StringType,true), StructField(&quot;prod_name&quot;,StringType,true), StructField(&quot;prod_price&quot;,DoubleType,true), //StructField(&quot;prod_price&quot;,StringType,true), //StructField(&quot;prod_price&quot;,IntegerType,true), StructField(&quot;prod_desc&quot;,StringType,true)//介绍 ) val productsSchema = StructType(productsFields) val productsDf = ssc.createDataFrame(productsRdd,productsSchema) productsDf.registerTempTable(&quot;Products&quot;) productsDf.printSchema() productsDf.show() ssc.sql(&quot;select prod_name from Products&quot;).show ssc.sql(&quot;select prod_id, prod_name,prod_price from Products&quot;).show ssc.sql(&quot;select * from Products&quot;).show ssc.sql(&quot;select vend_id from Products&quot;).show ssc.sql(&quot;select distinct vend_id from Products&quot;).show //大小写敏感；作用于其后多列 ssc.sql(&quot;select prod_name from Products limit 5&quot;).show ssc.sql(&quot;select prod_id, prod_name from Products order by prod_id desc,prod_price&quot;).show //select 语句的最后一个子句 ；作用于其后多列（可分别指定排序方式）；排序字段可为非显示列 ssc.sql(&quot;select prod_id, prod_name from Products order by 22 desc,prod_price&quot;).show //select 语句的最后一个子句 ；作用于其后多列（可分别指定排序方式）；排序字段可为非显示列 ssc.sql(&quot;select * from Products where prod_price = 3.49&quot;).show ssc.sql(&quot;select * from Products where prod_price !&lt; 3.49&quot;).show ssc.sql(&quot;select * from Products where prod_price &lt;= 5&quot;).show ssc.sql(&quot;select * from Products where vend_id != &apos;DLL01&apos;&quot;).show //字符串类型比较，使用单引号 ssc.sql(&quot;select * from Products where prod_price between 5 and 10&quot;).show ssc.sql(&quot;select * from Products where prod_price is null &quot;).show ssc.sql(&quot;select * from Products where prod_price is not null &quot;).show ssc.sql(&quot;select * from Products where vend_id = &apos;DLL01&apos; or vend_id = &apos;BRS01&apos;&quot;).show ssc.sql(&quot;select * from Products where vend_id = &apos;DLL01&apos; or vend_id = &apos;BRS01&apos; and prod_price &gt; 10&quot;).show ssc.sql(&quot;select * from Products where (vend_id = &apos;DLL01&apos; or vend_id = &apos;BRS01&apos;) and prod_price &gt; 10&quot;).show ssc.sql(&quot;select * from Products where vend_id in (&apos;DLL01&apos;,&apos;BRS01&apos;)&quot;).show //建议优先使用in ssc.sql(&quot;select * from Products where vend_id in (&apos;DLL01&apos;,&apos;BRS01&apos;) and prod_price &gt; 10&quot;).show ssc.sql(&quot;select * from Products where vend_id not in (&apos;DLL01&apos;,&apos;BRS01&apos;)&quot;).show //建议优先使用in ssc.sql(&quot;select * from Products where prod_name like &apos;%doll%&apos;&quot;).show ssc.sql(&quot;select * from Products where prod_price like &apos;%4%&apos;&quot;).show //不仅仅字符能用通配符 ssc.sql(&quot;select * from Products where prod_name like &apos;__ inch teddy bear&apos;&quot;).show ssc.sql(&quot;select * from Products where prod_name like &apos;% inch teddy bear&apos;&quot;).show ////ssc.sql(&quot;select * from Products where prod_name like &apos;1[2,8] inch teddy bear&apos;&quot;).show //不支持[] ssc.sql(&quot;select prod_name + &apos;-&apos; + prod_id from Products&quot;).show // 不支持列拼接 ssc.sql(&quot;select prod_name ,prod_price,prod_price*2 from Products&quot;).show // 执行运算 + - * / ssc.sql(&quot;select prod_name ,prod_price,prod_price%2 from Products&quot;).show // 执行运算 + - * / ssc.sql(&quot;select lower(prod_name),upper(prod_name),prod_price from Products&quot;).show // ssc.sql(&quot;select trim(prod_name) from Products&quot;).show // 不支持去空格 ssc.sql(&quot;select prod_price, abs(prod_price), sqrt(prod_price) from Products&quot;).show ssc.sql(&quot;select avg(prod_price) avg_price from Products&quot;).show //avg忽略为空的行 ，作用于字符串返回null ssc.sql(&quot;select count(prod_price) c from Products&quot;).show //不忽略为空的行 ssc.sql(&quot;select max(prod_price) m from Products&quot;).show // 同min也可以作用于字符串列，分别返回第一行最后一行 ssc.sql(&quot;select sum(prod_name) s from Products&quot;).show //作用于字符串返回0.0 ssc.sql(&quot;select sum(distinct prod_price),count(distinct prod_price)from Products&quot;).show //avg，max，min 不支持去重后求值 ssc.sql(&quot;select vend_id ,count(*),max(prod_price),avg(prod_price) from Products group by vend_id&quot;).show ssc.sql(&quot;select vend_id ,count(*) c,max(prod_price) m,avg(prod_price) from Products where prod_price &gt; 0 group by vend_id having m &gt; 5 and c &gt; 2&quot;).show ssc.sql(&quot;select vend_id ,count(*) c,max(prod_price) m,avg(prod_price) from Products where prod_price &gt; 0 group by vend_id having m &gt; 5 order by c3&quot;).show ssc.sql(&quot;select prod_name p from Products&quot;).show // 别名 ssc.sql(&quot;select prod_name,upper(prod_name),prod_price from Products&quot;).show // val vendorsData = Array( &quot;BRS01,Bears R Us,123 Main Street,Bear Town,MI,44444,USA&quot;, &quot;BRE02,Bear Emporium,500 Park Street,Anytown,OH,44333,USA&quot;, &quot;DLL01,Doll House Inc.,555 High Street,Dollsville,CA,99999,USA&quot;, &quot;FRB01,Furball Inc.,1000 5th Avenue,New York,NY,11111,USA&quot;, &quot;FNG01,Fun and Games,42 Galaxy Road,London,NULL,N16 6PS,England&quot;, &quot;JTS01,Jouets et ours,1 Rue Amusement,Paris,NULL,45678,France&quot; ) val vendorsRdd = sc.parallelize(vendorsData).map(_.split(&quot;,&quot;)).map(p =&gt; Row(p(0), p(1), p(2), p(3), p(4), p(5), p(6))) val vendorsFields = Array( StructField(&quot;vend_id&quot;,StringType,false), StructField(&quot;vend_name&quot;,StringType,false), StructField(&quot;vend_address&quot;,StringType,true), StructField(&quot;vend_city&quot;,StringType,true), StructField(&quot;vend_state&quot;,StringType,true), StructField(&quot;vend_zip&quot;,StringType,true), StructField(&quot;vend_country&quot;,StringType,true)) val vendorsSchema = StructType(vendorsFields) val vendorsDf = ssc.createDataFrame(vendorsRdd,vendorsSchema) vendorsDf.registerTempTable(&quot;Vendors&quot;) vendorsDf.printSchema() vendorsDf.show() ssc.sql(&quot;select vend_id from Products group by vend_id having avg(prod_price) &gt; 3&quot;).show // ssc.sql(&quot;select * from Vendors where vend_id in (select vend_id from Products group by vend_id having avg(prod_price) &gt; 3)&quot;).show //subquery在2.0支持https://issues.apache.org/jira/browse/SPARK-4226 ssc.sql(&quot;select vend_name,prod_name,prod_price from Vendors v,Products p where v.vend_id = p.vend_id &quot;).show ssc.sql(&quot;select vend_name,prod_name,prod_price from Vendors v inner join Products p on v.vend_id = p.vend_id &quot;).show ssc.sql(&quot;select vend_name,prod_name,prod_price from Vendors v left join Products p on v.vend_id = p.vend_id &quot;).show ssc.sql(&quot;select vend_name,prod_name,prod_price from Vendors v right outer join Products p on v.vend_id = p.vend_id &quot;).show ssc.sql(&quot;select vend_name,prod_name,prod_price from Vendors v full outer join Products p on v.vend_id = p.vend_id &quot;).show ssc.sql(&quot;select vend_name ,count(prod_id) c from Products p left outer join Vendors v on v.vend_id = p.vend_id group by vend_name&quot;).show ssc.sql(&quot;select * from Vendors v where vend_id = &apos;JTS01&apos; or vend_state= &apos;MI&apos;&quot;).unionAll(ssc.sql(&quot;select * from Vendors v where v.vend_state in (&apos;MI&apos;,&apos;OH&apos;)&quot;)).show ssc.sql(&quot;select * from Vendors v where vend_id = &apos;JTS01&apos; or vend_state= &apos;MI&apos; union select * from Vendors v where v.vend_state in (&apos;MI&apos;,&apos;OH&apos;)&quot;).show ssc.sql(&quot;select * from Vendors v where vend_id = &apos;JTS01&apos; or vend_state= &apos;MI&apos; union all select * from Vendors v where v.vend_state in (&apos;MI&apos;,&apos;OH&apos;)&quot;).show ssc.sql(&quot;INSERT into table Vendors select * from Vendors&quot;).show &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"让Spark Streaming在YARN上长时间运行","slug":"大数据/Spark/让Spark Streaming在YARN上长时间运行","date":"2016-05-16T08:48:05.000Z","updated":"2018-11-29T11:57:39.716Z","comments":true,"path":"2016/05/16/大数据/Spark/让Spark Streaming在YARN上长时间运行/","link":"","permalink":"http://yoursite.com/2016/05/16/大数据/Spark/让Spark Streaming在YARN上长时间运行/","excerpt":"","text":"对于长时间运行的Spark Streaming作业，一旦提交到YARN群集便需要永久运行，直到有意停止。任何中断都会引起严重的处理延迟，并可能导致数据丢失或重复。YARN和Apache Spark都不是为了执行长时间运行的服务而设计的。但是，它们已经成功地满足了近实时数据处理作业的常驻需求。成功并不一定意味着没有技术挑战。 这篇博客总结了在安全的YARN集群上，运行一个关键任务且长时间的Spark Streaming作业的经验。您将学习如何将Spark Streaming应用程序提交到YARN群集，以避免在值班时候的不眠之夜。 Fault tolerance在YARN集群模式下，Spark驱动程序与Application Master（应用程序分配的第一个YARN容器）在同一容器中运行。此过程负责从YARN 驱动应用程序和请求资源（Spark执行程序）。重要的是，Application Master消除了在应用程序生命周期中运行的任何其他进程的需要。即使一个提交Spark Streaming作业的边缘Hadoop节点失败，应用程序也不会受到影响。 要以集群模式运行Spark Streaming应用程序，请确保为spark-submit命令提供以下参数：1spark-submit --master yarn --deploy-mode cluster 由于Spark驱动程序和Application Master共享一个JVM，Spark驱动程序中的任何错误都会阻止我们长期运行的工作。幸运的是，可以配置重新运行应用程序的最大尝试次数。设置比默认值2更高的值是合理的（从YARN集群属性yarn.resourcemanager.am.max尝试中导出）。对我来说，4工作相当好，即使失败的原因是永久性的，较高的值也可能导致不必要的重新启动。1spark-submit --master yarn --deploy-mode cluster --conf spark.yarn.maxAppAttempts=4 如果应用程序运行数天或数周，而不重新启动或重新部署在高度使用的群集上，则可能在几个小时内耗尽4次尝试。为了避免这种情况，尝试计数器应该在每个小时都重置。1spark-submit --master yarn --deploy-mode cluster --conf spark.yarn.maxAppAttempts=4 --conf spark.yarn.am.attemptFailuresValidityInterval=1h 另一个重要的设置是在应用程序发生故障之前executor失败的最大数量。默认情况下是max（2 * num executors，3），非常适合批处理作业，但不适用于长时间运行的作业。该属性具有相应的有效期间，也应设置。1spark-submit --master yarn --deploy-mode cluster --conf spark.yarn.maxAppAttempts=4 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.max.executor.failures=&#123;8 * num_executors&#125; --conf spark.yarn.executor.failuresValidityInterval=1h 对于长时间运行的作业，您也可以考虑在放弃作业之前提高任务失败的最大数量。默认情况下，任务将重试4次，然后作业失败。12spark-submit --master yarn --deploy-mode cluster --conf spark.yarn.maxAppAttempts=4 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.max.executor.failures=&#123;8 * num_executors&#125; --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.task.maxFailures=8Performance 当Spark Streaming应用程序提交到集群时，必须定义运行作业的YARN队列。我强烈建议使用YARN Capacity Scheduler并将长时间运行的作业提交到单独的队列。没有一个单独的YARN队列，您的长时间运行的工作迟早将被的大量Hive查询抢占。1spark-submit --master yarn --deploy-mode cluster --conf spark.yarn.maxAppAttempts=4 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.max.executor.failures=&#123;8 * num_executors&#125; --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.task.maxFailures=8 --queue realtime_queue Spark Streaming工作的另一个重要问题是保持处理时间的稳定性和高度可预测性。处理时间应保持在批次持续时间以下以避免延误。我发现Spark的推测执行有很多帮助，特别是在繁忙的群集中。当启用推测性执行时，批处理时间更加稳定。只有当Spark操作是幂等时，才能启用推测模式。1spark-submit --master yarn --deploy-mode cluster --conf spark.yarn.maxAppAttempts=4 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.max.executor.failures=&#123;8 * num_executors&#125; --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.task.maxFailures=8 --queue realtime_queue --conf spark.speculation=true Security在安全的HDFS群集上，长时间运行的Spark Streaming作业由于Kerberos票据到期而失败。没有其他设置，当Spark Streaming作业提交到集群时，会发布Kerberos票证。当票证到期时Spark Streaming作业不能再从HDFS写入或读取数据。 在理论上（基于文档），应该将Kerberos主体和keytab作为spark-submit命令传递：1spark-submit --master yarn --deploy-mode cluster --conf spark.yarn.maxAppAttempts=4 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.max.executor.failures=&#123;8 * num_executors&#125; --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.task.maxFailures=8 --queue realtime_queue --conf spark.speculation=true --principal user/hostname@domain --keytab /path/to/foo.keytab 实际上，由于几个错误（HDFS-9276, SPARK-11182）必须禁用HDFS缓存。如果没有，Spark将无法从HDFS上的文件读取更新的令牌。1spark-submit --master yarn --deploy-mode cluster --conf spark.yarn.maxAppAttempts=4 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.max.executor.failures=&#123;8 * num_executors&#125; --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.task.maxFailures=8 --queue realtime_queue --conf spark.speculation=true --principal user/hostname@domain --keytab /path/to/foo.keytab --conf spark.hadoop.fs.hdfs.impl.disable.cache=true Mark Grover指出，这些错误只影响在HA模式下配置了NameNodes的HDFS集群。谢谢，Mark。 Logging访问Spark应用程序日志的最简单方法是配置Log4j控制台追加程序，等待应用程序终止并使用yarn logs -applicationId [applicationId]命令。不幸的是终止长时间运行的Spark Streaming作业来访问日志是不可行的。 我建议安装和配置Elastic，Logstash和Kibana（ELK套装）。ELK的安装和配置是超出了这篇博客的范围，但请记住记录以下上下文字段：1234YARN application idYARN container hostnameExecutor id (Spark driver is always 000001, Spark executors start from 000002)YARN attempt (to check how many times Spark driver has been restarted) Log4j配置使用Logstash特定的appender和布局定义应该传递给spark-submit命令：1spark-submit --master yarn --deploy-mode cluster --conf spark.yarn.maxAppAttempts=4 --conf spark.yarn.am.attemptFailuresValidityInterval=1h --conf spark.yarn.max.executor.failures=&#123;8 * num_executors&#125; --conf spark.yarn.executor.failuresValidityInterval=1h --conf spark.task.maxFailures=8 --queue realtime_queue --conf spark.speculation=true --principal user/hostname@domain --keytab /path/to/foo.keytab --conf spark.hadoop.fs.hdfs.impl.disable.cache=true --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties --conf spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties --files /path/to/log4j.properties 最后，Spark Job的Kibana仪表板可能如下所示： Monitoring长时间运行的工作全天候运行，所以了解历史指标很重要。Spark UI仅在有限数量的批次中保留统计信息，并且在重新启动后，所有度量标准都消失了。再次，需要外部工具。我建议安装Graphite用于收集指标和Grafana来建立仪表板。 首先，Spark需要配置为将指标报告给Graphite，准备metrics.properties文件：123456*.sink.graphite.class=org.apache.spark.metrics.sink.GraphiteSink*.sink.graphite.host=[hostname]*.sink.graphite.port=[port] *.sink.graphite.prefix=some_meaningful_namedriver.source.jvm.class=org.apache.spark.metrics.source.JvmSourceexecutor.source.jvm.class=org.apache.spark.metrics.source.JvmSource Graceful stop最后一个难题是如何以优雅的方式停止部署在YARN上的Spark Streaming应用程序。停止（甚至杀死）YARN应用程序的标准方法是使用命令yarn application -kill [applicationId]。这个命令会停止Spark Streaming应用程序，但这可能发生在批处理中。因此，如果该作业是从Kafka读取数据然后在HDFS上保存处理结果，并最终提交Kafka偏移量，当作业在提交偏移之前停止工作时，您应该预见到HDFS会有重复的数据。 解决优雅关机问题的第一个尝试是在关闭程序时回调Spark Streaming Context的停止方法。123sys.addShutdownHook &#123; streamingContext.stop(stopSparkContext = true, stopGracefully = true)&#125; 令人失望的是，由于Spark应用程序几乎立即被杀死，一个退出回调函数来不及完成已启动的批处理任务。此外，不能保证JVM会调用shutdown hook。 在撰写本博客文章时，唯一确认的YARN Spark Streaming应用程序的确切方法是通知应用程序关于计划关闭，然后以编程方式停止流式传输（但不是关闭挂钩）。命令yarn application -kill 如果通知应用程序在定义的超时后没有停止，则应该仅用作最后手段。 可以使用HDFS上的标记文件（最简单的方法）或使用驱动程序上公开的简单Socket / HTTP端点（复杂方式）通知应用程序。 因为我喜欢KISS原理，下面你可以找到shell脚本伪代码，用于启动/停止Spark Streaming应用程序使用标记文件：12345678910111213start() &#123; hdfs dfs -touchz /path/to/marker/my_job_unique_name spark-submit ...&#125;stop() &#123; hdfs dfs -rm /path/to/marker/my_job_unique_name force_kill=true application_id=$(yarn application -list | grep -oe &quot;application_[0-9]*_[0-9]*&quot;`) for i in `seq 1 10`; do application_status=$(yarn application -status $&#123;application_id&#125; | grep &quot;State : \\(RUNNING\\|ACCEPTED\\)&quot;) if [ -n &quot;$application_status&quot; ]; then sleep 60s else force_kill=false break fi done $force_kill &amp;&amp; yarn application -kill $&#123;application_id&#125;&#125; 在Spark Streaming应用程序中，后台线程应该监视标记文件，当文件消失时停止上下文调用1streamingContext.stop(stopSparkContext = true, stopGracefully = true) Summary可以看到，部署在YARN上的关键任务Spark Streaming应用程序的配置相当复杂。以上提出的技术，由一些非常聪明的开发人员经过漫长而冗长乏味的迭代学习。最终，部署在高可用的YARN集群上的长期运行的Spark Streaming应用非常稳定。 翻译：http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"Kafka数据同步(镜像)工具:kafka mirror maker","slug":"大数据/Kafka数据同步(镜像)工具:kafka mirror maker","date":"2016-04-02T08:48:05.000Z","updated":"2020-03-29T10:17:58.391Z","comments":true,"path":"2016/04/02/大数据/Kafka数据同步(镜像)工具:kafka mirror maker/","link":"","permalink":"http://yoursite.com/2016/04/02/大数据/Kafka数据同步(镜像)工具:kafka mirror maker/","excerpt":"","text":"背景公司数据收集后会写入kafka集群，近期涉及到机房搬迁，在完成机房搬迁移前，两个机房都有业务需要某些topic的数据，两种处理方案：1是数据写入时候双写 2是老机房数据写入完成后再同步至新机房kafka集群。本文介绍kafka自带的集群镜像工具MirrorMaker，实现kafka集群间的数据同步。 一、概括来说MirrorMaker就是kafka生产者与消费者的一个整合，通过consumer从源Kafka集群消费数据，然后通过producer将数据重新推送到目标Kafka集群，如下图：二、MirrorMaker的使用相对也比较简单，下面说下启动命令及相关配置启动脚本在$KAFKA_HOME/bin目录下，可通过命令kafka-run-class.sh kafka.tools.MirrorMaker查看相关说明： 说明：whitelist、blacklist：该工具可以同步源集群所有的或者部分topic，可以用白名单描述要同步的topic，用黑名单描述不需要同步的topic，多个topic直接逗号分隔，并且支持通配符(java http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html)consumer.config：配置源kafka集群消费者相关信息 12zookeeper.connect=zk1ip1:2181,zk1ip2:2181/kafka/group.id=mirrorMaker producer.config :配置目标kafka集群生产者相关信息12metadata.broker.list=b1:9092,b2:9092compression.codec=none 启动命令：1sh $KAFKA_HOME/bin/kafka-run-class.sh kafka.tools.MirrorMaker --consumer.config $KAFKA_HOME/config/mirrorMakerConsumer.config --num.streams 2 --producer.config $KAFKA_HOME/config/amirrorMakerProducer.config —num.producers 2 --whitelist=&quot;topic2mirror&quot;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"Spark简介","slug":"大数据/Spark/Spark简介","date":"2016-03-29T08:48:05.000Z","updated":"2018-11-20T15:05:19.978Z","comments":true,"path":"2016/03/29/大数据/Spark/Spark简介/","link":"","permalink":"http://yoursite.com/2016/03/29/大数据/Spark/Spark简介/","excerpt":"","text":"历史背景Spark由2009加州大学伯克利分校AMP实验室(UC Berkeley AMP(Algorithms, Machines, and People Lab) lab)）所开发，并与2010年开源的，类似Hadoop MapReduce的通用、并行计算框架。Spark与MapReduce最大的不同是Job的中间输出和结果可以保存在内存中，从而减少读写HDFS的开销，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代计算的场景。Spark在2013年6月进入Apache，进入高速发展期，第三方开发者贡献了大量的代码，活跃度非常高，孵化8个月后成为Apache顶级项目。 主要特点 运行速度快Spark拥有DAG执行引擎，支持在内存中对数据进行迭代计算。官方提供的数据表明，如果数据由磁盘读取，速度是Hadoop MapReduce的10倍以上，如果数据从内存中读取，速度可以高达100倍以上。 使用方便Spark支持Scala、Java、Python等语言进行编写应用程序，支持SQL进行数据处理。特别是Scala，其是一种高效、可拓展的语言，能够用简洁的代码处理较为复杂的处理工作。 通用性强Spark提供了完整而强大的组件,提供批量计算、流式计算、图计算、机器学习等能力。 多执行环境Spark具有很强的适应性，能够以HDFS、Cassandra、HBase、S3和Techyon等为持久层的读写源，能够以Mesos、YARN和自身携带的Standalone模式作为资源管理器调度、执行j任务。 包含组件 Spark Core Spark引入了RDD (Resilient Distributed Dataset) 的概念，它是分布在一些节点中的只读数据对象集合，如果数据集一部分丢失，则可以根据“血统”对它们进行重建，保证了数据的高容错性。 提供了有向无环图（DAG）的分布式并行计算框架，并提供Cache机制来支持多次迭代计算或者数据共享，大大减少迭代计算之间读取数据局的开销，这对于需要进行多次迭代的数据挖掘和分析性能有很大提升。 移动计算而非移动数据，RDD Partition可以就近读取分布式文件系统中的数据块到各个节点内存中进行计算。 可以使用多线程池模型来减少task启动开稍。 采用容错的、高可伸缩性的akka作为通讯框架。 Spark SQL 是一个可以与Spark Core一起使用的组件，支持使用SQL处理结构化的数据，RDD、Hive数据。引入了新的RDD类型SchemaRDD（DataFram），可以象传统数据库定义表一样来定义SchemaRDD。SchemaRDD可以从RDD转换过来，也可以从Parquet文件读入，也可以使用HiveQL从Hive中获取。 内嵌了Catalyst查询优化框架，在把SQL解析成逻辑执行计划之后，利用Catalyst包里的一些类和接口，执行了一些简单的执行计划优化，最后变成RDD的计算 在应用程序中可以混合使用不同来源的数据，如可以将来自HiveQL的数据和来自RDD的数据进行Join操作。 Spark Streaming 是一个对实时数据进行流式处理的、高容错、高吞吐量的系统，可以对接Kafka、Flume、TCP等多种类型数据源，并对其进行类似Map、Reduce和Join等复杂操作，并支持将结果存储到HDFS、S3等存储文件系统，HBase、MySQL等数据库或应用到实时仪表盘。 并不是真正意义上的流式处理，而是微批，也是对Spark Core核心APId扩展，会将处理的数据流抽象为Dstream，DStream本质上表示RDD的序列，所以任何对DStream的操作都会转变为对底层RDD的操作。 MLlibSpark目前有两个机器学习包，ML和MLlib。 MLlib是Spark的机器学习组件之一，是一套用Spark编写的机器学习和统计算法。 Spark ML仍处于早期阶段，但自Spark 1.2以来，它提供了比MLlib更高级别的API，可帮助用户更轻松地创建实用的机器学习管道。 Spark MLLib主要建立在RDD之上，而ML建立在SparkSQL数据框之上。 4最终，Spark社区计划转向ML并弃用MLlib。 Spark ML和MLLib有一些独特的性能考虑因素，特别是在处理大数据量和高速缓存时，我们在???中介绍了一些。 GraphX是一个构建在Spark之上的图形处理框架，带有用于图形计算的API。 Graph X是Spark中最不成熟的组件之一，因此我们不会详细介绍它。 在Spark的未来版本中，类型图功能将开始在数据集API之上引入。 我们将粗略地看一下???中的图X. GraphX是Spark中用于图(e.g., Web-Graphs and Social Networks)和图并行计算(e.g., PageRank and Collaborative Filtering)的API,可以认为是GraphLab(C++)和Pregel(C++)在Spark(Scala)上的重写及优化，跟其他分布式图计算框架相比，GraphX最大的贡献是，在Spark之上提供一栈式数据解决方案，可以方便且高效地完成图计算的一整套流水作业。GraphX最先是伯克利AMPLAB的一个分布式图计算框架项目，后来整合到Spark中成为一个核心组件。 1.对Graph视图的所有操作，最终都会转换成其关联的Table视图的RDD操作来完成。这样对一个图的计算，最终在逻辑上，等价于一系列RDD的转换过程。因此，Graph最终具备了RDD的3个关键特性：Immutable、Distributed和Fault-Tolerant。其中最关键的是Immutable（不变性）。逻辑上，所有图的转换和操作都产生了一个新图；物理上，GraphX会有一定程度的不变顶点和边的复用优化，对用户透明。 2.两种视图底层共用的物理数据，由RDD[Vertex-Partition]和RDD[EdgePartition]这两个RDD组成。点和边实际都不是以表Collection[tuple]的形式存储的，而是由VertexPartition/EdgePartition在内部存储一个带索引结构的分片数据块，以加速不同视图下的遍历速度。不变的索引结构在RDD转换过程中是共用的，降低了计算和存储开销。 3.图的分布式存储采用点分割模式，而且使用partitionBy方法，由用户指定不同的划分策略（PartitionStrategy）。划分策略会将边分配到各个EdgePartition，顶点Master分配到各个VertexPartition，EdgePartition也会缓存本地边关联点的Ghost副本。划分策略的不同会影响到所需要缓存的Ghost副本数量，以及每个EdgePartition分配的边的均衡程度，需要根据图的结构特征选取最佳策略。目前有EdgePartition2d、EdgePartition1d、RandomVertexCut和CanonicalRandomVertexCut这四种策略。在淘宝大部分场景下，EdgePartition2d效果最好。 参考：http://spark.apache.org/","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/tags/其他/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}]},{"title":"一组图详解元数据、主数据与参考数据","slug":"大数据/一组图详解元数据、主数据与参考数据","date":"2016-03-17T08:48:05.000Z","updated":"2018-10-22T05:59:17.272Z","comments":true,"path":"2016/03/17/大数据/一组图详解元数据、主数据与参考数据/","link":"","permalink":"http://yoursite.com/2016/03/17/大数据/一组图详解元数据、主数据与参考数据/","excerpt":"","text":"在数据资产管理领域，有着许多相似的概念和词汇。譬如说“数据管理”和“数据治理”，像孪生兄弟一样让人纠结不已。上周，与一个朋友聊起元数据、主数据和参考数据的关系是什么。这个话题我们足足聊了二十分钟。这三个概念我在一开始做数据管理相关工作的时候也纠结了挺久，于是我根据聊起来的内容稍稍总结了一下，就有了这篇文章，希望能给读者减少些许疑惑。 1、假设场景 我们的假设场景先是这样的，现在正在为中国地理协会设计一个中国地理信息系统（当然真实的地理信息系统不会是这样，都说只是假设一下）。我现在正设计到“地市”这个对象。常说“千言万语不及一图”，这三者的关系咱们先上图。2、元数据–数据的数据 元数据（meta-data）是描述企业数据的相关数据，指在IT系统建设过程中所产生的有关数据定义，目标定义，转换规则等相关的关键数据，包括对数据的业务、结构、定义、存储、安全等各方面对数据的描述。 例如在假设场景中，我们设计了地市表的数据模型（如上图红色框里面表示），地市表这个实体的数据模型如何进行定义正是元数据所关心的范畴。 元数据可以说是企业的数据地图，它直接反映了企业中有什么样的数据，数据是如何存放的，例如，数据结构是什么样子，数据与业务之间的关系是怎么样，数据与数据之间的关系是怎么样，数据有什么样的安全需求，数据有什么样的存储需求。 针对元数据的管理，对于传统企业数据而言是非常重要的一项管理挑战。因为传统企业技术和管理观念上有所缺失，从而导致了许多问题。因此，我们在进行许多传统企业数据治理或者数据管理项目，也就是元数据管理方面时，常常会先从数据模型梳理着手。 3、主数据–企业黄金数据记录 主数据（main data）主要是指经实例化的企业关键数据。 还是回到我们的假设场景，我们在上面设计完成数据模型设计的“城市表”中填写了相应的城市数据，例如，北京、上海、广州、南宁等等。这些在城市表中填充的数据，正是组织中国地理协会的主数据，因为这些数据是中国地理协会这个组织的关键业务实体，它为组织的业务开展提供关联环境，而且它可能在企业业务开展过程中被反复引用。针对这些核心关键数据，组织和企业无论从数据的质量、一致性、可用性、管理规范等方面都应该有着最严格的数据要求。 那么一般而言，以下涉及企业经营的人、财、物的数据最有可能纳入企业主数据管理的范畴，例如 企业产品及其相关信息：包括企业相关产品、服务、版本、价格、标准操作等等 企业财务信息：包括业务、预算、利润、合同、财务科目等等 企业相关利益相关者：如客户、供应商、合作伙伴、竞争对手等 企业组织架构：如员工、部门等 可见，主数据就是企业被不同运营场合反复引用关键的状态数据，它需要在企业范围内保持高度一致。它可以随着企业的经营活动而改变，例如，客户的增加，组织架构的调整，产品下线等；但是，主数据的变化频率应该是较低的。所以，企业运营过程产生过程数据，如生产过程产生各种如订购记录、消费记录等，一般不会纳入主数据的范围。当然，在不同行业，不同企业对主数据有不同的看法和做法，正如我们与国内大型航空企业的实施相关数据项目时，也在为航班动态是不是主数据而纠结不已。 因此，有鉴于主数据对于企业的重要性，企业和组织需要对其主数据进行有效的管理：包括理解主数据应用需求，识别主数据来源及源头，梳理主数据上下游关系，数据整合和发布，提升主数据的数据质量等。 4、参考数据–数据的字典 在本文引用的假设案例中，我们将会注意到刚才填写的地市这类数据有些列，如省份、城市类型等。如果没有缺少上下文的环境，我们是无法理解其具体含义，这时候我们往往引入参考数据（reference data）加以解释和理解，如下图红色标注所示。 参考数据是增加数据可读性、可维护性以及后续应用的重要数据。例如，你看到“性别”的这个字段，很可能是1代表男性、2代表女性。在许多企业中有这样的约定俗成，而更多的参考数据可能记录在开发人员和运营人员的大脑当中。但问题是一旦这些人离开，您系统里面的数据就成了一堆没有注释的天书。 大家可能觉得，这所谓参考数据不就是数据字典吗？对，我们在很多系统里面都会有这样和那样的数据字典。但是正是由于这些数据字典局仅限于个别系统而没有统一标准，从一个侧面间接造就了大量的数据孤岛。企业为了进行更有效率的数据整合、数据共享和数据分析应用，开始尝试对参考数据进行企业或者部门层面的整合和管理，利用参考数据集记录系统尝试为范围内的IT系统中的数据库提供统一的参考数据。 5、小结 主数据则是真实的企业业务数据，是企业的关键业务数据。 参考数据则是对数据的解释，针对一些数据范围和取值的数据解释，让人们容易读取相关的数据。 元数据是对数据的描述，用于描述企业数据的所有信息和数据，如结构、关系、安全需求等，除增加数据可读性外，也是后续数据管理的基础。 一般而言，企业中这三类数据与其它数据的数据量、质量需求，更新频率、数据生命周期的关系大致如下图： 原文地址：http://www.cbdio.com/BigData/2016-02/16/content_4617126.htm","categories":[{"name":"数仓","slug":"数仓","permalink":"http://yoursite.com/categories/数仓/"}],"tags":[{"name":"数仓","slug":"数仓","permalink":"http://yoursite.com/tags/数仓/"}],"keywords":[{"name":"数仓","slug":"数仓","permalink":"http://yoursite.com/categories/数仓/"}]},{"title":"HBase 1.1.2 split 策略","slug":"大数据/HBase/HBase 1.1.2 split 策略","date":"2015-11-01T08:48:05.000Z","updated":"2018-11-10T07:40:11.732Z","comments":true,"path":"2015/11/01/大数据/HBase/HBase 1.1.2 split 策略/","link":"","permalink":"http://yoursite.com/2015/11/01/大数据/HBase/HBase 1.1.2 split 策略/","excerpt":"","text":"背景：今天同事用ycsb做HBase的性能测试，反馈说reigon总是在配置的大小前split(配置的是10G)，于是我就给他说起了hbase的spilt策略：从0.94增加了新的策略，还是在会每次flush的时候会去判断需不需要split，但是判断的策略有了改变，会比较现有文件的大小与改表region个数的平方memstore大小的关系，如果前者较大也会去做split……他打断说：region个数的平方memstore这个不对吧，日志里是memstore二倍的大小，还给我截图为证，无奈，就翻看了1.1.2的代码，发下算法真的变了，对应变成了region个数的三次方2memstore。 日志如下：1regionserver.IncreasingToUpperBoundRegionSplitPolicy: ShouldSplit because 0 size=296442912, sizeToCheck=268435456, regionsWithCommonTable=1 从日志中可以看到，每次flush时候都会调用IncreasingToUpperBoundRegionSplitPolicy类中的shouldSplit方法，方法内容如下： 12345678910111213141516171819202122232425262728@Override protected boolean shouldSplit() &#123; if (region.shouldForceSplit()) return true; boolean foundABigStore = false; // Get count of regions that have the same common table as this.region int tableRegionsCount = getCountOfCommonTableRegions(); // Get size to check long sizeToCheck = getSizeToCheck(tableRegionsCount); for (Store store : region.getStores()) &#123; // If any of the stores is unable to split (eg they contain reference files) // then don't split if ((!store.canSplit())) &#123; return false; &#125; // Mark if any store is big enough long size = store.getSize(); if (size &gt; sizeToCheck) &#123; LOG.debug(\"ShouldSplit because \" + store.getColumnFamilyName() + \" size=\" + size + \", sizeToCheck=\" + sizeToCheck + \", regionsWithCommonTable=\" + tableRegionsCount); foundABigStore = true; &#125; &#125; return foundABigStore; &#125; 代码说明： 首先，通过getCountOfCommonTableRegions()方法获取目前的region个数tableRegionCount，然后通过getSizeTOCheck(tableRegionCount)方法运算得出一个阈值sizeToCheck,接着在for循环中遍历该reigon下所有的sotre，如果有store不能做split(调用HStore类的canSplit方法，该方法判断store下的hfile是否有被reference的，即region刚拆分，但hfile还处于reference状态，未完成拆分)，直接返回false。如果该store可以做split，则比较store下hfile的大小与sizeToCheck的值，如果大于则标识foundABigStore置为true。 接着看下getSizeTOCheck(tableRegionCount)方法：123456protected long getSizeToCheck(final int tableRegionsCount) &#123; // safety check for 100 to avoid numerical overflow in extreme cases return tableRegionsCount == 0 || tableRegionsCount &gt; 100 ? getDesiredMaxFileSize(): Math.min(getDesiredMaxFileSize(), this.initialSize * tableRegionsCount * tableRegionsCount * tableRegionsCount);&#125; 代码说明： 如果tableRegionCount的值是0或者大于100，则通过getDesiredMaxFileSize()方法读取配置文件中的hbase.hregion.max.filesize值(即前文说的10G)，否则进行Math.min判断，后面tableRegionCount三次方很容易理解，看看initialSize怎么来的，相关方法内容如下： 123456789101112131415161718@Override protected void configureForRegion(HRegion region) &#123; super.configureForRegion(region); Configuration conf = getConf(); this.initialSize = conf.getLong(&quot;hbase.increasing.policy.initial.size&quot;, -1); if (this.initialSize &gt; 0) &#123; return; &#125; HTableDescriptor desc = region.getTableDesc(); if (desc != null) &#123; this.initialSize = 2*desc.getMemStoreFlushSize(); &#125; if (this.initialSize &lt;= 0) &#123; this.initialSize = 2*conf.getLong(HConstants.HREGION_MEMSTORE_FLUSH_SIZE, HTableDescriptor.DEFAULT_MEMSTORE_FLUSH_SIZE); &#125; &#125; 代码说明： 代码逻辑也是非常简单，这里不再赘述。 补充，昨天以为getCountOfCommonTableRegions()的逻辑是获取这张表所有的region，今天测试发现不是这样，回头再看代码，代码内容如下： 123456789101112131415 private int getCountOfCommonTableRegions() &#123; RegionServerServices rss = this.region.getRegionServerServices(); // Can be null in tests if (rss == null) return 0; TableName tablename = this.region.getTableDesc().getTableName(); int tableRegionsCount = 0; try &#123; List&lt;Region&gt; hri = rss.getOnlineRegions(tablename); tableRegionsCount = hri == null || hri.isEmpty()? 0: hri.size(); &#125; catch (IOException e) &#123; LOG.debug(&quot;Failed getOnlineRegions &quot; + tablename, e); &#125; return tableRegionsCount; &#125;&#125; 代码说明: 首先获取该region所在的regionserver，然后获取该regionserver上的所有region，而不是该表在整个集群中的region数量。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"HBase 1.0 Java 客户端 api(hbase 1.0 增删改查，建表、删表等)","slug":"大数据/HBase/HBase 1.0 Java 客户端 api(hbase 1.0 增删改查，建表、删表等)","date":"2015-09-02T08:48:05.000Z","updated":"2018-09-12T07:57:54.610Z","comments":true,"path":"2015/09/02/大数据/HBase/HBase 1.0 Java 客户端 api(hbase 1.0 增删改查，建表、删表等)/","link":"","permalink":"http://yoursite.com/2015/09/02/大数据/HBase/HBase 1.0 Java 客户端 api(hbase 1.0 增删改查，建表、删表等)/","excerpt":"","text":"说明：1.第一部分为代码2.第二部分为工程pom文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;/** * Created by xuemin on 15/9/28. */public class HBaseTest &#123; public static Configuration configuration; public static Connection connection; public static Admin admin; public static void main(String[] args) throws IOException &#123; createTable(\"t2\",new String[]&#123;\"cf1\",\"cf2\"&#125;); insterRow(\"t2\", \"rw1\", \"cf1\", \"q1\", \"val1\"); getData(\"t2\", \"rw1\", \"cf1\", \"q1\"); scanData(\"t2\", \"rw1\", \"rw2\"); deleRow(\"t2\",\"rw1\",\"cf1\",\"q1\"); deleteTable(\"t2\"); &#125; //初始化链接 public static void init()&#123; configuration = HBaseConfiguration.create(); configuration.set(\"hbase.zookeeper.quorum\",\"10.10.3.181,10.10.3.182,10.10.3.183\"); configuration.set(\"hbase.zookeeper.property.clientPort\",\"2181\"); configuration.set(\"zookeeper.znode.parent\",\"/hbase\"); try &#123; connection = ConnectionFactory.createConnection(configuration); admin = connection.getAdmin(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; //关闭连接 public static void close()&#123; try &#123; if(null != admin) admin.close(); if(null != connection) connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; //建表 public static void createTable(String tableNmae,String[] cols) throws IOException &#123; init(); TableName tableName = TableName.valueOf(tableNmae); if(admin.tableExists(tableName))&#123; System.out.println(\"talbe is exists!\"); &#125;else &#123; HTableDescriptor hTableDescriptor = new HTableDescriptor(tableName); for(String col:cols)&#123; HColumnDescriptor hColumnDescriptor = new HColumnDescriptor(col); hTableDescriptor.addFamily(hColumnDescriptor); &#125; admin.createTable(hTableDescriptor); &#125; close(); &#125; //删表 public static void deleteTable(String tableName) throws IOException &#123; init(); TableName tn = TableName.valueOf(tableName); if (admin.tableExists(tn)) &#123; admin.disableTable(tn); admin.deleteTable(tn); &#125; close(); &#125; //查看已有表 public static void listTables() throws IOException &#123; init(); HTableDescriptor hTableDescriptors[] = admin.listTables(); for(HTableDescriptor hTableDescriptor :hTableDescriptors)&#123; System.out.println(hTableDescriptor.getNameAsString()); &#125; close(); &#125; //插入数据 public static void insterRow(String tableName,String rowkey,String colFamily,String col,String val) throws IOException &#123; init(); Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowkey)); put.addColumn(Bytes.toBytes(colFamily), Bytes.toBytes(col), Bytes.toBytes(val)); table.put(put); //批量插入 /* List&lt;Put&gt; putList = new ArrayList&lt;Put&gt;(); puts.add(put); table.put(putList);*/ table.close(); close(); &#125; //删除数据 public static void deleRow(String tableName,String rowkey,String colFamily,String col) throws IOException &#123; init(); Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowkey)); //删除指定列族 //delete.addFamily(Bytes.toBytes(colFamily)); //删除指定列 //delete.addColumn(Bytes.toBytes(colFamily),Bytes.toBytes(col)); table.delete(delete); //批量删除 /* List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;(); deleteList.add(delete); table.delete(deleteList);*/ table.close(); close(); &#125; //根据rowkey查找数据 public static void getData(String tableName,String rowkey,String colFamily,String col)throws IOException&#123; init(); Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowkey)); //获取指定列族数据 //get.addFamily(Bytes.toBytes(colFamily)); //获取指定列数据 //get.addColumn(Bytes.toBytes(colFamily),Bytes.toBytes(col)); Result result = table.get(get); showCell(result); table.close(); close(); &#125; //格式化输出 public static void showCell(Result result)&#123; Cell[] cells = result.rawCells(); for(Cell cell:cells)&#123; System.out.println(\"RowName:\"+new String(CellUtil.cloneRow(cell))+\" \"); System.out.println(\"Timetamp:\"+cell.getTimestamp()+\" \"); System.out.println(\"column Family:\"+new String(CellUtil.cloneFamily(cell))+\" \"); System.out.println(\"row Name:\"+new String(CellUtil.cloneQualifier(cell))+\" \"); System.out.println(\"value:\"+new String(CellUtil.cloneValue(cell))+\" \"); &#125; &#125; //批量查找数据 public static void scanData(String tableName,String startRow,String stopRow)throws IOException&#123; init(); Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); //scan.setStartRow(Bytes.toBytes(startRow)); //scan.setStopRow(Bytes.toBytes(stopRow)); ResultScanner resultScanner = table.getScanner(scan); for(Result result : resultScanner)&#123; showCell(result); &#125; table.close(); close(); &#125;&#125; 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;td&lt;/groupId&gt; &lt;artifactId&gt;hbase_test&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!-- &lt;hadoop.version&gt;2.6.0-cdh5.4.5&lt;/hadoop.version&gt;--&gt; &lt;hbase.version&gt;1.0&lt;/hbase.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"Atom 常用插件推荐","slug":"工程、工具/Atom-常用插件推荐","date":"2015-08-16T08:48:05.000Z","updated":"2018-09-25T01:24:44.076Z","comments":true,"path":"2015/08/16/工程、工具/Atom-常用插件推荐/","link":"","permalink":"http://yoursite.com/2015/08/16/工程、工具/Atom-常用插件推荐/","excerpt":"","text":"一、背景Atom 是github专门为程序员推出的一个跨平台文本编辑器，于2015年1月8日开源，官方slogan为”A hackable text editor for the 21st Century”，其相对其他IDE算是比较轻量的，启动速度比较快，也提供了很多功能插件和主题。其用户界面简洁、直观，支持Windows、Mac、Linux 三大桌面平台，原生支持HTML、JavaScript、CSS、Node.js等前端应用编程语言，并且有Java、C#、PHP等语言的插件。相对其他IDE，其还有个特点，就是完全免费。 我没依赖其进行项目开发，就不在此详细聊其功能。前面文章[《Atom – MarkDown编辑、预览利器》]（https://zhangxuemin.cn/2015/06/10/Atom-%E9%A2%84%E8%A7%88%E5%92%8C%E5%90%8C%E6%AD%A5%E6%BB%9A%E5%8A%A8/）推荐了一些和MarkDown相关的插件，本问为其他一些常用的插件推荐。 二、简要设置（for Mac用户）2.1调整主题 2.2安装插件 三、常用插件3.1列式选择插件 Sublime-Style-Column-Selection如名字一样，完全与Sublime的功能一样，包括快捷键。 3.2命令行插件 platformio-ide-terminal有了它，使用iTerm的频率也降低了。 3.3汉化插件 simplified-chinese-menu汉化语义一般，下载尝试后就disable了 3.4码字时有震撼效果 activate-power-mode配合机械键盘与要飞起的感觉，只是时间长了会头晕，尝试后disable的第二个插接…… 3.4选择内容相同部分高亮 highlight-selected","categories":[{"name":"工程、工具","slug":"工程、工具","permalink":"http://yoursite.com/categories/工程、工具/"}],"tags":[{"name":"Atom","slug":"Atom","permalink":"http://yoursite.com/tags/Atom/"},{"name":"MarkDown","slug":"MarkDown","permalink":"http://yoursite.com/tags/MarkDown/"}],"keywords":[{"name":"工程、工具","slug":"工程、工具","permalink":"http://yoursite.com/categories/工程、工具/"}]},{"title":"Atom -- MarkDown编辑、预览利器","slug":"工程、工具/Atom-预览和同步滚动","date":"2015-06-10T08:48:05.000Z","updated":"2018-10-27T08:04:44.962Z","comments":true,"path":"2015/06/10/工程、工具/Atom-预览和同步滚动/","link":"","permalink":"http://yoursite.com/2015/06/10/工程、工具/Atom-预览和同步滚动/","excerpt":"","text":"一、背景Atom是github专门为程序员推出的一个跨平台文本编辑器，于2015年1月8日开源，官方slogan为”A hackable text editor for the 21st Century”，其相对其他IDE算是比较轻量的，启动速度比较快，也提供了很多功能插件和主题。其用户界面简洁、直观，支持Windows、Mac、Linux 三大桌面平台，原生支持HTML、JavaScript、CSS、Node.js等前端应用编程语言，并且有Java、C#、PHP等语言的插件。相对其他IDE，其还有个特点，就是完全免费。 我没依赖其进行项目开发，就不在此详细聊其功能。我主要使用MarkDown功能，相对其他MarkDown编辑器，其支持类似Sublime等其他IDE项目管理的方式管理MarkDown文件，并且语言完全支持GitHub Flavored Markdown。本文主要聊下Atom对MarkDown的支持及优化（插件化的功能），以及遇到的问题。 二、简要设置（for Mac用户）2.1调整主题 2.2安装插件 三、MarkDown功能插件3.1增强预览 markdown-preview-plusAtom自带的Markdown预览插件markdown-preview功能比较简单，markdown-preview-plus对其做了功能扩展和增强。 支持预览实时渲染。(Ctrl + Shift + M) 支持Latex公式。(Ctrl + Shift + X) 使用该插件前，需要先禁用markdown-preview。 3.2同步滚动 markdown-scroll-sync配合预览功能使用，预览后需要修改某个地方时候，能够方便的找到代码位置。 3.3代码增强(language-markdown)一般的MarkDown编辑器都会提供代码高亮功能，初此外该插件提供了快捷生产代码等功能。 3.4其他插件以上是几个与MarkDown比较相关的几个插件，Atom官方还提供了很多其他功能插件，详见：https://atom.io/packages 四、遇到的问题按如上方式安装完markdown-scroll-sync后，有如下提示：12TypeError: Right-hand side of &apos;instanceof&apos; is not callable at /packages/markdown-scroll-sync/lib/main.coffee:38:14 在官网issues中找到原因是与markdown-preview-plus有版本冲突，解决方案是卸载高版本的plus插件，命令行安装指定版本，命令如下：1apm install markdown-preview-plus@2.4.16 五、参考https://github.com/vincentcn/markdown-scroll-sync/issues/334","categories":[{"name":"工程、工具","slug":"工程、工具","permalink":"http://yoursite.com/categories/工程、工具/"}],"tags":[{"name":"Atom","slug":"Atom","permalink":"http://yoursite.com/tags/Atom/"},{"name":"MarkDown","slug":"MarkDown","permalink":"http://yoursite.com/tags/MarkDown/"}],"keywords":[{"name":"工程、工具","slug":"工程、工具","permalink":"http://yoursite.com/categories/工程、工具/"}]},{"title":"简单、高效的数据结构--Bloom Filter(布隆过滤器)","slug":"编程基础/简单、高效的数据结构--Bloom Filter(布隆过滤器)","date":"2015-04-25T08:48:05.000Z","updated":"2018-11-19T15:14:08.297Z","comments":true,"path":"2015/04/25/编程基础/简单、高效的数据结构--Bloom Filter(布隆过滤器)/","link":"","permalink":"http://yoursite.com/2015/04/25/编程基础/简单、高效的数据结构--Bloom Filter(布隆过滤器)/","excerpt":"","text":"背景：在翻看吴军博士《数学之美》的时候看到布隆过滤器，所以在此总结下自己对其的认识。 一、布隆过滤器用来做什么布隆过滤器可用来判定一个元素是否属于一个集合，更严谨的讲是：它能100%确定一个元素不属于某个集合，但不能100%确定一个元素属于某个集合。 关于其使用场景，第一想到的是用来判定“是否需要执行高昂的操作”，比如访问网络或者磁盘上的某些资源。比如Google 的BitTable 和Apach HBase都使用布隆过滤器判断查询的数据是否存，来确定是否需要继续读取磁盘。再比如，用爬虫抓取网页时，有些网页会相互链接或者多个网页含有同一网页链接，所以使用布隆过滤器判断url是否爬取过了，来确定是否继续发起该url的访问。 二、布隆过滤器是怎么实现和使用的布隆过滤器由两个部分组成：一个位数组和一组散列函数。为了初始化布隆过滤器，我们将位数组中的所有位设置为零。 所以，如果我们有一个十位数组：1[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 当为集合增加一个元素时，我们将元素作为输入提供给散列函数。 每个散列函数将输出一个数组索引。 假设将字符串“hello”传递给两个散列函数f1，f2，这两个散列函数给出索引0和4，我们将位数组中的相应位设置为1：1[1, 0, 0, 0, 1, 0, 0, 0, 0, 0] 当查询一个元素时，我们将元素传给两个散列函数，获得两个索引后，检查数组中相应位的值： 如果两个值中有0，可能的索引值组合有（0，0）、（0，1）、（1，0），即可判定该元素不在集合中。所以，不一定需要检查所有函数返回位的值，如果发现至少有一个值是0，那么即可判定该元素不在集合中。比如我们要查询“word”是否属于集合，假设两个函数返回的索引是1和5。因为两个索引位值都是0，所以检查其中任意一个位都可以得出“word不属于集合“的结论。 如果两个值都是1，只可判定为“该元素可能在集合中”，因为散列函数可能会产生冲突。比如我们使用两个函数获取“bloom”的索引可能为1和9，获取“filter”的索引可能为5和7，而此时再去查询“word”，会因为1和9已被“bloom”和“filter”已经设置为1而产出冲突。因此，我们不能100%确定查询的元素在集合中。 当去除一个元素集合时，因为“散列函数可能产生冲突”的问题，如果我们要重置希望删除元素的相应位，可能会误删具有相同索引位的其他元素，所以此时不需要（不容易）对位数组进行处理。 三、为什么布隆过滤器效率比较高时间复杂度添加元素时，由于不需要迭代位数组，而是简单的设置索引位的值，所以操作所花费的时间仅取决于散列函数的个数，所以对于对于k个哈希函数的布隆过滤器，添加元素的时间复杂度为O(k) 。查询元素时，对于k个哈希函数的布隆过滤器，只需要在位数组中检查的索引数量有一个不变的上界，所以查询元素的时间复杂度也为O(k)。 空间复杂度由于不需要存储元素，只需依赖一定长度的位数组判断是否存在，并且数组长度的大小不也取决于集合中元素的多少，可以在误判率变大或效率变低的代价下减少存储（位数组）。 四、布隆过滤器有哪些缺点主要缺点是有一定的误判率和删除比较困难，所以随着存入集合的元素的增加，误判率也随之增加。误判率大小和三个指标有关：位数组长度m、集合长度n、散列函数个数k，其之间关系可以参考文献 ，该文献证明了对于给定的m、n，当 k = ln(2)* m/n 时误判率是最小的。","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"}],"tags":[{"name":"Bloom Filter","slug":"Bloom-Filter","permalink":"http://yoursite.com/tags/Bloom-Filter/"}],"keywords":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"}]},{"title":"HDFS小文件处理--文件归档Archive","slug":"大数据/Hadoop/HDFS小文件处理--文件归档Archive","date":"2015-04-02T08:48:05.000Z","updated":"2020-03-29T10:33:24.612Z","comments":true,"path":"2015/04/02/大数据/Hadoop/HDFS小文件处理--文件归档Archive/","link":"","permalink":"http://yoursite.com/2015/04/02/大数据/Hadoop/HDFS小文件处理--文件归档Archive/","excerpt":"","text":"Hadoop并不擅长对小型文件的储存，原因取决于Hadoop文件系统的文件管理机制，Hadoop的文件存储的单元为一个块（block），block的数据存放在集群中的datanode节点上，由namenode对所有datanode存储的block进行管理。namenode将所有block的元数据存放在内存中，以方便快速的响应客户端的请求。那么问题来了，不管一个文件有多小，Hadoop都把它视为一个block，大量的小文件，将会把namenode的内存耗尽。那么如何对大量的小文件进行有效的处理呢？Hadoop的优秀工程师们其实已经为我们考虑好了，Hadoop提供了一个叫Archive归档工具，Archive可以把多个文件归档成为一个文件，换个角度来看，Archive实现了文件的元数据整理，但是，归档的文件大小其实没有变化，只是压缩了文件的元数据大小。 归档小文件使用命令：hadoop archive -archiveName test.har -p /user/archive/test /user/har 参数含义：-archiveName 指定归档文件名；-p 指定要进行归档目录的父目录，支持同时归档多个子目录；/user/archive/test 归档文件的目录/user/har 归档文件存放的目录 查看归档文件har文件采用一套不同于hdfs的路径协议。基本格式为：har://&lt;hdfs路径&gt;。在未添加协议url时，我们看到的是har文件在HDFS中的底层形式，加上协议头后我们可以看到其目录结构与源目录结构相同。hadoop fs -ls -R har:///user/har/test.har 归档的不足1、archive不支持压缩，所以归档前后占用空间没有减少；2、archive一旦创建就不能进行修改；3、archive虽然解决了namenode的空间问题，但是，在执行mapreduce时，会把多个小文件交给同一个mapreduce去split，这会降低mapreduce的效率。 解除归档如果需要修改文件或者想提升mapreduce处理的效率，可以考虑将数据解除归档。hadoop distcp har:///user/har/test.har /user/tmp/test","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"HBase0.96 升级步骤及源代码分析","slug":"大数据/HBase/HBase0.96 升级步骤及源代码分析","date":"2014-11-12T08:48:05.000Z","updated":"2018-09-12T08:33:24.467Z","comments":true,"path":"2014/11/12/大数据/HBase/HBase0.96 升级步骤及源代码分析/","link":"","permalink":"http://yoursite.com/2014/11/12/大数据/HBase/HBase0.96 升级步骤及源代码分析/","excerpt":"","text":"测试的升级环境为cdh4.3 升级到社区版 Hadoop2.2/HBase0.96。 一、验证HDFS和Zookeeper已正常运行（HDFS and ZooKeeper must be up!） 二、在集群中任一服务器上执行hbase upgrade -check 1.下面是该命令的输出（官网给的例子，自己执行的时候输出不同，由于官网的有HFileV1的文件，故用之举例）1234567891011121314151617181920212223242526272829303132333435Tables Processed:hdfs://localhost:41020/myHBase/.META.hdfs://localhost:41020/myHBase/usertablehdfs://localhost:41020/myHBase/TestTablehdfs://localhost:41020/myHBase/tCount of HFileV1: 2HFileV1:hdfs://localhost:41020/myHBase/usertable /fa02dac1f38d03577bd0f7e666f12812/family/249450144068442524hdfs://localhost:41020/myHBase/usertable /ecdd3eaee2d2fcf8184ac025555bb2af/family/249450144068442512Count of corrupted files: 1Corrupted Files:hdfs://localhost:41020/myHBase/usertable/fa02dac1f38d03577bd0f7e666f12812/family/1Count of Regions with HFileV1: 2Regions to Major Compact:hdfs://localhost:41020/myHBase/usertable/fa02dac1f38d03577bd0f7e666f12812hdfs://localhost:41020/myHBase/usertable/ecdd3eaee2d2fcf8184ac025555bb2afThere are some HFileV1, or corrupt files (files with incorrect major version) HFileV1: 2HFileV1:hdfs://localhost:41020/myHBase/usertable /fa02dac1f38d03577bd0f7e666f12812/family/249450144068442524hdfs://localhost:41020/myHBase/usertable /ecdd3eaee2d2fcf8184ac025555bb2af/family/249450144068442512Count of corrupted files: 1Corrupted Files:hdfs://localhost:41020/myHBase/usertable/fa02dac1f38d03577bd0f7e666f12812/family/1Count of Regions with HFileV1: 2Regions to Major Compact:hdfs://localhost:41020/myHBase/usertable/fa02dac1f38d03577bd0f7e666f12812hdfs://localhost:41020/myHBase/usertable/ecdd3eaee2d2fcf8184ac025555bb2afThere are some HFileV1, or corrupt files (files with incorrect major version)` 2.说明：该命令的作用是检测HBase的数据文件（HDFS）是否有HFileV1类型的（HFile V1和V2是HDFS的两个版本，文件数据组织格式有些差别）和corrupted的。HBase从0.94开始支持HFileV2类型文件，0.94是两个版本都兼容，0.96则只兼容HFileV2，所以要处理遗留的HFileV1文件，处理方法是对这些HFileV1文件进行 major compact。corrupted文件可试着修复，数量不多或者数据完整性要求不高的话可以直接move了。确保该步结果如下图，再执行下面步骤。 三、在集群中任一服务器上执行hbase upgrade -execute 1.下面是该命令的输出12345678910111213Starting Namespace upgradeCreated version file at hdfs://localhost:41020/myHBase with version=7Migrating table testTable to hdfs://localhost:41020/myHBase/.data/default/testTable…..Created version file at hdfs://localhost:41020/myHBase with version=8Successfully completed NameSpace upgrade.Starting Znode upgrade….Successfully completed Znode upgradeStarting Log splitting…Successfully completed Log splitting 2.说明：该命令的作用是触发升级任务。从输出日志可以看出，升级涉及到三部分内容：1231）Namespace upgrade2）Znode upgrade3）Log upgrade 我在执行时抛出了如下异常：123456789101112131415161714/11/12 11:49:05 WARN wal.HLogSplitter: Could not open hdfs://XXX/hbase/WALs/XXX,60020,1415264007965/XXX%2C60020%2C1415264007965.1415618778662 for reading. File is emptyjava.io.EOFException at java.io.DataInputStream.readFully(DataInputStream.java:197) at java.io.DataInputStream.readFully(DataInputStream.java:169) at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1845) at org.apache.hadoop.io.SequenceFile$Reader.initialize(SequenceFile.java:1810) at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1759) at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1773) at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader.&lt;init&gt;(SequenceFileLogReader.java:69) at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.reset(SequenceFileLogReader.java:174) at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.initReader(SequenceFileLogReader.java:183) at org.apache.hadoop.hbase.regionserver.wal.ReaderBase.init(ReaderBase.java:68) at org.apache.hadoop.hbase.regionserver.wal.HLogFactory.createReader(HLogFactory.java:126) at org.apache.hadoop.hbase.regionserver.wal.HLogFactory.createReader(HLogFactory.java:89) at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:645) at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:554) at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:273) 从日志可以看出是Log upgrade时发现WAL日志不存在（HDFS上该文WALs目录确实为空），虽然有异常，但不影响升级结果，可忽略（有点不负责任的感觉，有时间了再找找原因）。 四、源代码分析具体执行升级任务的是org.apache.hadoop.hbase.migration包下的UpgradeTo96类（可从异常日志中看到，也可从两个命令跟踪到该类，有时间了再补一篇各命令脚本执行的顺序及调用类的文章），从run（）方法看起： 1、首先跟下check步骤： 可以看出相关的类是：HFileV1Detect\\or，看下其run（）方法： 先看看processResult（） 再跟checkFOrV1Files（）找找hFileV1Set 和 corruptedHFiles 跟进isTableDir()可以看出其是判断Path是否为文件 再看processTable() 它先调了processRegion（）： 在该方法里，遍历了region中列族的storefile文件，并对其进行版本判断，版本算法见computeMajorVersion() 2、再看下upgrade步骤 还记得前文提到的升级的三部分内容吧：1231）Namespace upgrade2）Znode upgrade3）Log upgrade 看下NamespaceUpgrade类的run（）： 跟进init() 定义了几个重要目录（由于0.94和0.96元数据组织上有很大差别，目录组织也有很大变化，这个就不截图了）：123/hbase/data/deflate（该目录下放的是用户表）/hbase/data/hbase （该目录下放的是meta、namespace数据）/hbase/.migration （Any artifacts left from migration can be moved here） 再看upgradeTableDirs():这个方法就是升级工作的具体内容，注释是我添上去的，就不一一跟进方法里了 1）makeNamespaceDirs() 创建 /hbase/data/hbase /hbase/data/deflate目录2）migrateTables() move用户表数据至/hbase/data/deflate目录，其实是调用了hdfs的rename方法3）migrateSnapshots() move快照文件，也是rename，.snapshot –&gt;.hbase-snapshot4）migrateDotDirs() Dot dirs to rename. Leave the tmp dir named ‘.tmp’ and snapshots as .hbase-snapshot.5）migrateMeta() move meta 表至 /hbase/data/hbase目录下，也是rename6）migrateACL() 同上7）deleteRoot() 删除root表 另外：官网说这个升级是不可逆的，但根据升级代码看是可以的，就是备份upgradeTableDirs（）方法中处理的目录，当然用户表不用备份，待测。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"HBase数据模型(Data Model)","slug":"大数据/HBase/HBase数据模型(Data Model)","date":"2014-10-02T08:48:05.000Z","updated":"2018-09-13T05:27:29.606Z","comments":true,"path":"2014/10/02/大数据/HBase/HBase数据模型(Data Model)/","link":"","permalink":"http://yoursite.com/2014/10/02/大数据/HBase/HBase数据模型(Data Model)/","excerpt":"","text":"在HBase中，数据存储在有行有列的二维表。HBase中的表是与关系型数据库重复的术语，但这个类比对我们理解HBase来说帮助不大。将HBase的表当做一个多层的Map，反而会可以容易理解。 HBase数据模型相关术语Table（表）一个HBase表由多行组成。 Row（行）HBase中的行里面包含一个行键和一个或者多个包含值的列。行按照行键的字母顺序存储在表中。因为这个原因，行键的设计就显得非常重要。数据的存储目标是相关的数据存储在一起。一个常见的行键模式是网站域名。如果你的行键是域名，你应该将域名进行反转(org.apache.www, org.apache.mail, org.apache.jira)再存储。这样的话，所有Apache域名将会存储在一起，好过基于子域名的首字母分散在各处。 Column（列）HBase中的列包含用“:”分隔开的列族和列限定符。 Column Family（列族）因为性能的原因，列族物理上包含一组列和它们的值。每一个列族拥有一系列的存储属性，例如值是否缓存在内存中，数据如何压缩或者他的行键是否要编码等等。表中的每一行拥有相同的列族，尽管一个给定的行可能没有存储任何数据在一个给定的列族中。 Column Qualifier（列限定符）列限定符将添加到列族中，以便为给定的数据提供索引。例如给定了一个列族content，那么限定符可能是content:html，也可以是content:pdf。虽然列族在创建表时是固定的，但列限定符是可变的，并且行与行之间可能有很大差异。 Cell（单元）单元是由行、列族、列限定符、值和代表值版本的时间戳组成的。 Timestamp（时间戳）时间戳与每个值一起写入，并且是数据给定版本的标识符。默认情况下，时间戳表示写入数据时RegionServer上的时间，但你也可以在写入数据时指定一个不同于RegionServer上的时间的时间戳。 概念视图可以读一下Jim R博客中一篇非常容易理解的解释HBase数据模型的文章Understanding HBase and BigTable。Amandeep Khurana的PDF Introduction to Basic Schema Design中提供了另一个很好的解释。 阅读不同观点的资料可能会帮助你更透彻地了解HBase的设计。链接的文章与本节中的信息具有相同的基础。 接下来的例子取自BigTable论文第二页中的例子，在此基础上做了些许的改变。一个名为webtable的表，表中有两行（com.cnn.www 和 com.example.www）和三个列族（contents, anchor, 和 people）。在这个例子当中，第一行(com.cnn.www)中列族anchor包含两列（anchor:cssnsi.com, anchor:my.look.ca），列族content包含一列（contents:html）。这个例子中com.cnn.www行拥有5个版本，而com.example.www行有一个版本。contents:html列中包含给定网页的整个HTML。anchor列族下的限定符，每个都包含链接到该行所代表的站点的外部站点，以及它在其链接的锚点中使用的文本。People列族代表与该网站相关联的人员。 列名12按照约定，一个列名的格式为列族名前缀加限定符。例如，列contents:html由列族contents和html限定符。冒号（:）用于将列族和列限定符分开。 Table webtable 在HBase中，表中的单元如果是空将不占用空间或者事实上不存在。这就使得HBase看起来“稀疏”。表视图不是唯一方式来查看HBase中数据，甚至不是最精确的。下面的方式以多层级Map的方式来表达相同的信息。下面只是一个用于说明目的的模型，可能不是百分百的精确。123456789101112131415161718192021222324&#123; &quot;com.cnn.www&quot;: &#123; contents: &#123; t6: contents:html: &quot;&lt;html&gt;...&quot; t5: contents:html: &quot;&lt;html&gt;...&quot; t3: contents:html: &quot;&lt;html&gt;...&quot; &#125; anchor: &#123; t9: anchor:cnnsi.com = &quot;CNN&quot; t8: anchor:my.look.ca = &quot;CNN.com&quot; &#125; people: &#123;&#125; &#125; &quot;com.example.www&quot;: &#123; contents: &#123; t5: contents:html: &quot;&lt;html&gt;...&quot; &#125; anchor: &#123;&#125; people: &#123; t5: people:author: &quot;John Doe&quot; &#125; &#125;&#125; 物理视图尽管一个概念层次的表看起来是由一些列稀疏的行组成，但他们是通过列族来存储的。一个新建的限定符(column_family:column_qualifier)可以随时地添加到已存在的列族中。 概念视图中的空单元实际上是没有进行存储的。因此对于返回时间戳为t8的contents:html的值的请求，结果为空。同样的，一个返回时间戳为t9的anchor:my.look.ca的值的请求，结果也为空。然而，如果没有指定时间戳的话，那么会返回特定列的最新值。对有多个版本的列，优先返回最新的值，因为时间戳是按照递减顺序存储的。因此对于一个返回com.cnn.www里面所有的列的值并且没有指定时间戳的请求，返回的结果会是时间戳为t6的contents:html的值、时间戳 为t9的anchor:cnnsi.com的值和时间戳为t8的anchor:my.look.ca的值。 关于Apache Hase如何存储数据的内部细节，查看regions.arch. 命名空间命名空间是类似于关系数据库系统中数据库表的逻辑分组。这个抽象的概念为即将到来的多租户相关特性奠定了基础： Quota Management (HBASE-8410) - Restrict the amount of resources (i.e. regions, tables) a namespace can consume. Namespace Security Administration (HBASE-9206) - Provide another level of security administration for tenants. Region server groups (HBASE-6721) - A namespace/table can be pinned onto a subset of RegionServers thus guaranteeing a course level of isolation. 命名空间管理命名空间可以被创建、移除和修改。命名空间成员资格是在表创建期间通过指定表单的完全限定表名来确定的：1&lt;table namespace&gt;:&lt;table qualifier&gt; Example 11. Examples1234567891011#Create a namespacecreate_namespace &apos;my_ns&apos;#create my_table in my_ns namespacecreate &apos;my_ns:my_table&apos;, &apos;fam&apos;#drop namespacedrop_namespace &apos;my_ns&apos;#alter namespacealter_namespace &apos;my_ns&apos;, &#123;METHOD =&gt; &apos;set&apos;, &apos;PROPERTY_NAME&apos; =&gt; &apos;PROPERTY_VALUE&apos;&#125; 预定义命名空间有两种预定义的特殊的命名空间 hbase – 系统命名空间, 用于包含HBase内部表 default – 没有明确指定命名空间的表将会自动落入这个命名空间 Example 12. Examples12345#namespace=foo and table qualifier=barcreate &apos;foo:bar&apos;, &apos;fam&apos;#namespace=default and table qualifier=barcreate &apos;bar&apos;, &apos;fam&apos; 22. 表在模式定义时预先声明表。 23. 行行键是未解释的字节。行是按照字典顺序进行排序的并且最小的排在前面。空的字节数据用来表示表的命名空间的开头和结尾。 24. 列族列在HBase中是归入到列族里面的。一个列的所有列成员都涌向相同的前缀。例如，列courses:history和cources:math是cources列族的成员，冒号用于将列族和列限定符分开。列族前缀必须由可打印的字符组成。列限定符可以由任意字节组成。列族必须在结构定义阶段预先声明号而列则不需要再结构设计阶段预先定义而是可以在表的创建和运行阶段快速的加入。 物理存储上，所有的列族成员都存储在文件系统。由于调优和存储规范都是在列族层次上，建议所有的列族成员具有相同的通用的访问模式和大小特征。 25. 单元一个{row,column,version}完全指定了HBase的一个单元。单元内容是未解释的字节 26. 数据模型操作数据模型的四个主要操作是Get，Put，Scan和Delete。可以通过Table实例进行操作。 26.1. 获取Get 返回指定行的属性。Gets通过Table.get.执行。 26.2. 插入Put 操作是在行键不存在时添加新行或者行键已经存在时进行更新。 Puts 是通过 Table.put (写缓存) 或者Table.batch (没有写缓存)执行的。 26.3. 扫描Scan 允许迭代多行以获取指定的属性。 下面是表实例中Scan的例子。假设一个表里面有”row1”, “row2”, “row3”，然后有另外一组行键为”abc1”, “abc2”,和”abc3”。下面的例子展示如何设置一个Scan实例来返回以“row”开头的行。12345678910111213141516public static final byte[] CF = \"cf\".getBytes();public static final byte[] ATTR = \"attr\".getBytes();...Table table = ... // instantiate a Table instanceScan scan = new Scan();scan.addColumn(CF, ATTR);scan.setRowPrefixFilter(Bytes.toBytes(\"row\"));ResultScanner rs = table.getScanner(scan);try &#123; for (Result r = rs.next(); r != null; r = rs.next()) &#123; // process result... &#125;&#125; finally &#123; rs.close(); // always close the ResultScanner!&#125; 请注意，通常，为扫描指定特定停止点的最简单方法是使用InclusiveStopFilter类。 26.4. 删除Delete 操作是将一个行从表中移除. Deletes 通过 Table.delete执行。HBase不会立刻对数据的进行操作，而是为死亡数据创建一个称为墓碑的标签。这个墓碑和死亡数据会在“major compations”中被清除。查看 version.delete 获取更多关于列的版本删除的信息，查看 compaction 获取关于compations的更多信息。 27. 版本在HBase通过{row, column, version}完全指定一个单元。可以有无限数量的单元格，他们行和列相同，但单元格地址仅在其版本维度上有所不同。 行和列使用字节来表达，而版本是通过长整型来指定的。典型来说，这个long类型的时间实例就像java.util.Date.getTime() 或者 System.currentTimeMillis()返回的一样，以毫秒为单位，返回当前时间和January 1, 1970 UTC的时间差。 HBase的版本维度以递减顺序存储，以致读取一个存储的文件时，返回的是最新版本的数据。 在HBase中，对单元版本的语义存在很多困惑。 尤其是： 如果多个数据写到一个具有相同版本的单元里，只能获取到最后写入的那个 以非递增的版本顺序写入也是可以的。 下面我们将描述HBase中版本维度是如何运作的。可以看 HBASE-2406 关于HBase版本的讨论。 Bending time in HBase 是关于HBase的版本或者时间维度的好读物。它提供了比这里更多的关于版本的细节信息。正如这里写到的，这里提到的覆盖存在的时间戳的限制将不再存在。这部分只是Bruno Dumon所写的关于版本的基本大纲。 27.1.指定版本的存储数量版本的最大存储数量是列结构的一个部分并且在表创建时指定，或者通过alter命令行，或者通过 HColumnDescriptor.DEFAULT_VERSIONS来修改。HBase0.96之前，默认数量是3，HBase0.96之后改为1. Example 13. 修改一个列族的最大版本数.123这个例子使用HBase Shell来修改列族 f1的最大版本数为5，你也可以使用 HColumnDescriptor来实现。hbase&gt; alter ‘t1′, NAME =&gt; ‘f1′, VERSIONS =&gt; 5 Example 14. 修改一个列族的最小版本数Modify123你也可以通过指定最小半本书来存储列族。默认情况下，该值为零，意味着这个属性是禁用的。下面的例子是通过HBase Shell设置列族f1中的所有列的最小版本数为2。你也可以通过 HColumnDescriptor来实现。hbase&gt; alter ‘t1′, NAME =&gt; ‘f1′, MIN_VERSIONS =&gt; 2 从HBase0.98.2开始，你可以通过设定在hbase-site.xml中设置hbase.column.max.version属性为所有新建的列指定一个全局的默认的最大版本数。 27.2. 版本和HBase 操作在这部分我们来看一下版本维度在HBase的每个核心操作中的表现。 27.2.1. Get/ScanGet是通过获取Scan的第一个数据来实现的。下面的讨论适用于 Get 和 Scans.。 默认情况下，如果你没有指定明确的版本，当你执行一个Get操作时，那个版本为最大值的单元将被返回（可能是也可能不是最新写人的那个）。默认的行为可以通过下面方式来修改： 返回不止一个版本 查看 Get.setMaxVersions() 返回最新版本以外的版本, 查看 Get.setTimeRange() 想要获得小于或等于固定值的最新版本，仅仅通过使用一个0到期望版本的范围和设置最大版本数为1就可以实现获得一个特定时间点的最新版本的记录。 27.2.2. 默认的Get 例子下面例子仅仅返回行的当前版本。123456public static final byte[] CF = \"cf\".getBytes();public static final byte[] ATTR = \"attr\".getBytes();...Get get = new Get(Bytes.toBytes(\"row1\"));Result r = table.get(get);byte[] b = r.getValue(CF, ATTR); // returns current version of value 27.2.3. Get版本的例子下面是获得行的最新3个版本的例子：12345678public static final byte[] CF = \"cf\".getBytes();public static final byte[] ATTR = \"attr\".getBytes();...Get get = new Get(Bytes.toBytes(\"row1\"));get.setMaxVersions(3); // will return last 3 versions of rowResult r = table.get(get);byte[] b = r.getValue(CF, ATTR); // returns current version of valueList&lt;KeyValue&gt; kv = r.getColumn(CF, ATTR); // returns all versions of this column 27.2.4. PutPut操作常常是以固定的时间戳来创建一个新单元。默认情况下，系统使用服务的 currentTimeMillis，但是你也可以为每一个列自己指定版本（长整型）。这就意味着你可以指定一个过去或者未来的时间点，或者不是时间格式的长整型。 为了覆盖已经存在的值，对和那个你想要覆盖的单元完全一样的row、column和version进行put操作。 隐式版本例子下面Put是以当前时间为版本的隐式操作123456public static final byte[] CF = \"cf\".getBytes();public static final byte[] ATTR = \"attr\".getBytes();...Put put = new Put(Bytes.toBytes(row))put.add(CF, ATTR, Bytes.toBytes( data));table.put(put); 显示版本例子下面的put是显示指定时间戳的操作。1234567public static final byte[] CF = &quot;cf&quot;.getBytes();public static final byte[] ATTR = &quot;attr&quot;.getBytes();...Put put = new Put( Bytes.toBytes(row));long explicitTimeInMs = 555; // just an exampleput.add(CF, ATTR, explicitTimeInMs, Bytes.toBytes(data));table.put(put); 警告: 版本时间戳是HBase内部用来计算数据的存活时间的。它最好避免自己设置。最好是将时间戳作为行的单独属性或者作为key的一部分，或者两者都有。 27.2.5. Delete有三种不同的删除类型。可以看看Lars Hofhansl所写的博客 Scanning in HBase: Prefix Delete Marker. Delete:列的指定版本 Delete column:列的所有版本 Delete family:特定列族里面的所有列。当要删除整个行时，HBase将会在内部为每一个列族创建一个墓碑。 删除通过创建一个墓碑标签来工作的。例如，让我们来设想我们要删除一个行。为此你可指定一个版本，或者使用默认的currentTimeMillis 。这就是删除小于等于该版本的所有单元。HBase不会修改数据，例如删除操作将不会立刻删除满足删除条件的文件。相反的，称为墓碑的会被写入，用来掩饰被删除的数据。当HBase执行一个压缩操作，墓碑将会执行一个真正地删除死亡值和墓碑自己的删除操作。如果你的删除操作指定的版本大于目前所有的版本，那么可以认为是删除整个行的数据。 你可以在 Put w/timestamp → Deleteall → Put w/ timestamp fails 用户邮件列表中查看关于删除和版本之间的相互影响的有益信息。keyvalue也可以到keyvalue查看更多关于内部KeyValue格式的信息。 删除标签会在下一次仓库压缩操作中被清理掉，除非为列族设置了 KEEP_DELETED_CELLS (查看 Keeping Deleted Cells)。为了保证删除时间的可配置性，你可以通过在 hbase-site.xml.中hbase.hstore.time.to.purge.deletes属性来设置TTL（生存时间）。如果 hbase.hstore.time.to.purge.deletes没有设置或者设置为0，所有的删除标签包括哪些墓碑都会在下一次精简操作中被干掉。此外，未来带有时间戳的删除标签将会保持到发生在hbase.hstore.time.to.purge.deletes加上代表标签的时间戳的时间和的下一次精简操作。 27.3. 当前的局限性27.3.1. Deletes mask Puts删除覆盖插入/更新删除操作覆盖插入/更新操作，即使put在delete之后执行的。可以查看 HBASE-2256. 还记得一个删除写入一个墓碑，只有当下一次精简操作发生时才会执行真正地删除操作。假设你执行了一个删除全部小于等于T的操作。在此之外又做了一个时间戳为T的put操作。这个put操作即使是发生在delete之后，也会被delete墓碑所覆盖。执行put的时候不会报错，不过当你执行一个get的时候会发现执行无效。你会在精简操作之后重新开始工作。如果你在put的使用的递增的版本，那么这些问题将不会出现。但如果你不在意时间，在执行delelte后立刻执行put的话，那么它们将有可能发生在同一时间点，这将会导致上述问题的出现。 27.3.2. 精简操作影响查询结果创建三个版本为t1,t2,t3的单元，并且设置最大版本数为2.所以当我们查询所有版本时，只会返回t2和t3。但是当你删除版本t2和t3的时候，版本t1会重新出现。显然，一旦重要精简工作运行之后，这样的行为就不会再出现。（查看 Bendingtime in HBase.） 28. 排序次序HBase中所有的数据模型操作返回的数据都是经过排序的。首先是行排序，其次是列族，接着是列限定符，最后是时间戳（递减排序，最新的记录最先返回） 29. 列元数据所有列的元数据都存储在一个列族的内部KeyValue实例中。因此，HBsase不仅支持一行中有多列，而且支持行之间的列的差异多样化。跟踪列名是你的责任。 唯一获取一个列族的所有列的方法是处理所有的行。查看 keyvalue获得更多关于HBase内部如何存储数据的信息。 30. JoinsHBase是否支持join是一个常见的问题，答案是没有，至少没办法像RDBMS那样支持（例如等价式join或者外部join）。正如本章节所阐述的，HBase中读取数据的操作是Get和Scan。 然而，这不意味着等价式join功能没办法在你的应用中实现，但是你必须自己实现。两种主要策略是将数据非结构化地写到HBase中，或者查找表然后在应用中或者MapReduce代码中实现join操作（正如RDBMS所演示的，将根据表的大小会有几种不同的策略，例如嵌套使循环和hash-join）。哪个是最好的方法？这将依赖于你想做什么，没有一种方案能够应对各种情况。 31. ACID查看 ACID Semantics. Lars Hofhansl也写了一份报告 ACID in HBase.","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}]},{"title":"Java中的String，StringBuilder，StringBuffer三者的区别","slug":"编程基础/Java/Java中的String，StringBuilder，StringBuffer三者的区别","date":"2014-09-16T08:48:05.000Z","updated":"2020-03-29T10:12:11.889Z","comments":true,"path":"2014/09/16/编程基础/Java/Java中的String，StringBuilder，StringBuffer三者的区别/","link":"","permalink":"http://yoursite.com/2014/09/16/编程基础/Java/Java中的String，StringBuilder，StringBuffer三者的区别/","excerpt":"","text":"String，StringBuilder，StringBuffer三个类之间的区别主要是在两个方面：执行效率和线程安全。 执行效率从执行效率比较，三者的关系为StringBuilder &gt; StringBuffer &gt; String 。 String最慢的原因String为字符串常量，而StringBuilder和StringBuffer均为字符串变量，即String对象一旦创建之后该对象是不可更改的，但后两者的对象是变量，是可以更改的。以下面一段代码为例：1234String str=&quot;abc&quot;;System.out.println(str);str=str+&quot;de&quot;;System.out.println(str); 如果运行这段代码会发现先输出“abc”，然后又输出“abcde”，好像是str这个对象被更改了，其实，这只是一种假象罢了，JVM对于这几行代码是这样处理的，首先创建一个String对象str，并把“abc”赋值给str，然后在第三行中，其实JVM又创建了一个新的对象也名为str，然后再把原来的str的值和“de”加起来再赋值给新的str，而原来的str就会被JVM的垃圾回收机制（GC）给回收掉了，所以，str实际上并没有被更改，也就是前面说的String对象一旦创建之后就不可更改了。所以，Java中对String对象进行的操作实际上是一个不断创建新的对象并且将旧的对象回收的一个过程，所以执行速度很慢。 而StringBuilder和StringBuffer的对象是变量，对变量进行操作就是直接对该对象进行更改，而不进行创建和回收的操作，所以速度要比String快很多。 另外，有时候我们会这样对字符串进行赋值12341 String str=&quot;abc&quot;+&quot;de&quot;;2 StringBuilder stringBuilder=new StringBuilder().append(&quot;abc&quot;).append(&quot;de&quot;);3 System.out.println(str);4 System.out.println(stringBuilder.toString()); 这样输出结果也是“abcde”和“abcde”，但是String的速度却比StringBuilder的反应速度要快很多，这是因为第1行中的操作和1String str=&quot;abcde&quot;; 是完全一样的，所以会很快，而如果写成下面这种形式123String str1=&quot;abc&quot;;String str2=&quot;de&quot;;String str=str1+str2; 那么JVM就会像上面说的那样，不断的创建、回收对象来进行这个操作了。速度就会很慢。 线程安全在线程安全上，StringBuilder是线程不安全的，而StringBuffer是线程安全的如果一个StringBuffer对象在字符串缓冲区被多个线程使用时，StringBuffer中很多方法可以带有synchronized关键字，所以可以保证线程是安全的，但StringBuilder的方法则没有该关键字，所以不能保证线程安全，有可能会出现一些错误的操作。所以如果要进行的操作是多线程的，那么就要使用StringBuffer，但是在单线程的情况下，还是建议使用速度比较快的StringBuilder。 ##总结 String：适用于少量的字符串操作的情况 StringBuilder：适用于单线程下在字符缓冲区进行大量操作的情况 StringBuffer：适用多线程下在字符缓冲区进行大量操作的情况","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://yoursite.com/tags/JAVA/"}],"keywords":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"}]},{"title":"Java垃圾回收(2)--垃圾对象回收的方法","slug":"编程基础/Java/Java垃圾回收(2)--垃圾对象回收的方法","date":"2014-09-09T08:48:05.000Z","updated":"2018-11-10T14:20:52.031Z","comments":true,"path":"2014/09/09/编程基础/Java/Java垃圾回收(2)--垃圾对象回收的方法/","link":"","permalink":"http://yoursite.com/2014/09/09/编程基础/Java/Java垃圾回收(2)--垃圾对象回收的方法/","excerpt":"","text":"上篇文章简述了JVM GC的第一个阶段，垃圾对象的判别方法（使用可达性分析法找到所有存活的对象），本篇简要说明下GC的下一个阶段：对象回收的方法。 清除（sweep）清除算法最为简单，直接将垃圾对象所占内存标记为空闲内存（JVM会维护一个空闲列表），当需要为新创建对象分配内存时，内存管理模块会从空闲内存中寻址内存，并分配给新创建的对象。 缺点该方法维护了一个“空闲列表”，意味着增加了对象分配的开销。 一是会造成内存碎片。由于JVM堆中对象必须是连续分布的，因此可能出现总空闲内存足够，但是无法分配的极端情况。另一个则是分配效率较低。如果是一块连续的内存空间，那么我们可以通过指针加法（pointer bumping）来做分配。而对于空闲列表，Java 虚拟机则需要逐个访问列表中的项，来查找能够放入新建对象的空闲内存。每当进行扫描时，JVM必须确保可以重用填充了无法访问的对象的区域。 这可能（并最终会）导致内存碎片，与磁盘碎片类似，会导致两个问题：写操作变得更加耗时，因为找到足够大小的下一个空闲块不再是一个简单的操作。 因此，如果碎片升级到没有单个空闲片段足以容纳新创建的对象的点，则会发生分配错误。为避免此类问题，JVM确保碎片不会失控。 因此，垃圾收集过程中也会发生“内存碎片整理”过程，而不仅仅是标记和扫描。 此过程将所有可到达对象重新定位到彼此旁边，从而消除（或减少）碎片。 这是一个例子： 可能存在大量的自由区域但是如果没有单个区域足够大以容纳分配，则分配仍将失败（在Java中具有OutOfMemoryError）。 第二种是压缩（compact），即把存活的对象聚集到内存区域的起始位置，从而留下一段连续的内存空间。这种做法能够解决内存碎片化的问题，但代价是压缩算法的性能开销。 第三种则是复制（copy），即把内存区域分为两等分，分别用两个指针 from 和 to 来维护，并且只是用 from 指针指向的内存区域来分配内存。当发生垃圾回收时，便把存活的对象复制到 to 指针指向的内存区域中，并且交换 from 指针和 to 指针的内容。复制这种回收方式同样能够解决内存碎片化的问题，但是它的缺点也极其明显，即堆空间的使用效率极其低下。","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://yoursite.com/tags/JAVA/"}],"keywords":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"}]},{"title":"Java垃圾回收(1)--垃圾对象的判别方法","slug":"编程基础/Java/Java垃圾回收(1)--垃圾对象的判别方法","date":"2014-09-02T08:48:05.000Z","updated":"2018-11-10T13:27:02.563Z","comments":true,"path":"2014/09/02/编程基础/Java/Java垃圾回收(1)--垃圾对象的判别方法/","link":"","permalink":"http://yoursite.com/2014/09/02/编程基础/Java/Java垃圾回收(1)--垃圾对象的判别方法/","excerpt":"","text":"什么是垃圾回收同一些其他语言一样，Java语言中也有”Garbage Collection”的概念，国内一般将其译为“垃圾回收”，其中“垃圾”指的是存在于堆中的不再被使用的对象。如果不清理这些垃圾对象，那么这些垃圾对象所占的内存空间会一直被占用到应用程序结束，这些空间将不能被其他对象使用。而“垃圾回收”就是把那些不再被使用的对象进行清除，收回占用的内存空间。有些语言垃圾回收方式是手动回收，但如果写代码时候忘记了释放数据所占内存，那么该部分内存将无法被重复使用，这种情况一般被称为“内存泄漏”。Java语言(JVM)采用了“自动垃圾回收”，开发人员不再需要考虑自己清理回收垃圾，程序运行时将自动收集不再使用的内存并释放它。垃圾回收，顾名思义，做法是找到垃圾并将其回收。这篇主要聊一聊，如何找到、判别垃圾对象。 如何判别垃圾对象引用计数法其中一个比较古老的方法是“引用计数法（reference counting）”，他的做法是为每个对象设置一个“引用计数器”，用来表示该对象被引用的个数。如果一个对象的引用计数器值为0，则说明这个对象不会再被使用，应该被回收掉。如下图所示，其中圆圈表示内存中的对象，圈内的数字表示其引用计数。其中一些灰色圆圈的对象的计数器值为0，一些计数器值虽不为0，但只被计数器值为0的对象引用，一旦计数器为0的对象被回收，他们的计数器值也将为0。蓝色圆圈相反，指的是这些对象的引用计数器值不会为0。 缺点：需处理循环引用下图中红色圆圈就表明了“引用计数法”其中的一个缺点：无法正确处理“循环引用对象”的情况，导致内存泄漏。其另一个比较明显的缺点是：需要额外的空间来存储计数器，以及处理计数器繁琐的更新操作。 可达性分析法另一个方法，也就是JVM目前在使用的方法，叫“可达性分析法”。如下图，该算法引入了“GC Root”的概念，使用“绿色云”表示。一系列GC Roots作为初始的存活对象集合，然后再分别从集合中的对象出发，将集合中对象引用的对象也放到集合中(在下图中显示为蓝色的对象)。该过程一般被称为“标记（mark）”，最终未被放到集合的对象便是应被回收的垃圾对象(在下图中显示为灰色)。那什么是GC Root呢？我们可以简单理解为由堆外指向堆内的引用，常见的有以下几种： 当前执行方法的局部变量和输入参数 活动线程 加载类的静态字段 JNI引用 从下图中可以很明了的看到，所有GC Root都没法到达上图中那些红色的圆圈，所以上图中的红色圆圈对象都也能被回收。所以使用可达性分析可以解决引用计数法所不能解决的循环引用问题。 虽然可达性分析法比较容易理解，但在实际使用中也存在一些缺点，最突出的就是“Stop-the-world pause”。 缺点：Stop-the-world因为程序执行过程中，大多是多线程环境，内存会是一直“动态变化”的，在JVM使用可达性分析法进行垃圾回收标记时候，如果垃圾回收线程和我们的任务线程是一起执行的话，那么任务线程和垃圾回收线程可能会相互影响。为了避免相互影响，JVM使用了 Stop-the-world 模式，在标记过程中，JVM会停止其他非垃圾回收线程的工作，直到完成垃圾回收。这也就造成了垃圾回收所谓的暂停时间（GC pause）。其中暂停的时间长度，不取决于堆中对象的总数，也不取决于堆整体内存的大小，而取决于其中活动对象的数量，因为垃圾对象GC Root根本不可达。","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://yoursite.com/tags/JAVA/"}],"keywords":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"}]}]}